{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network Churn Rate model building\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "os.chdir(r'F:\\Machine learning\\Machine learning practice\\Minimize Churn rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               user         churn           age      deposits    withdrawal  \\\n",
      "count  26996.000000  26996.000000  26996.000000  26996.000000  26996.000000   \n",
      "mean   35418.535302      0.413913     32.219921      3.342051      0.307045   \n",
      "std    20319.620354      0.492542      9.964838      9.131992      1.055488   \n",
      "min        1.000000      0.000000     17.000000      0.000000      0.000000   \n",
      "25%    17808.750000      0.000000     25.000000      0.000000      0.000000   \n",
      "50%    35745.500000      0.000000     30.000000      0.000000      0.000000   \n",
      "75%    53236.750000      1.000000     37.000000      1.000000      0.000000   \n",
      "max    69658.000000      1.000000     91.000000     65.000000     29.000000   \n",
      "\n",
      "       purchases_partners     purchases      cc_taken  cc_recommended  \\\n",
      "count        26996.000000  26996.000000  26996.000000    26996.000000   \n",
      "mean            28.066677      3.273967      0.073789       92.639502   \n",
      "std             42.221432      8.953651      0.437331       88.868773   \n",
      "min              0.000000      0.000000      0.000000        0.000000   \n",
      "25%              0.000000      0.000000      0.000000       10.000000   \n",
      "50%              9.000000      0.000000      0.000000       65.000000   \n",
      "75%             43.000000      1.000000      0.000000      164.000000   \n",
      "max           1067.000000     63.000000     29.000000      522.000000   \n",
      "\n",
      "        cc_disliked      ...       android_user  registered_phones  \\\n",
      "count  26996.000000      ...       26996.000000       26996.000000   \n",
      "mean       0.050637      ...           0.587235           0.420988   \n",
      "std        0.871431      ...           0.492340           0.912884   \n",
      "min        0.000000      ...           0.000000           0.000000   \n",
      "25%        0.000000      ...           0.000000           0.000000   \n",
      "50%        0.000000      ...           1.000000           0.000000   \n",
      "75%        0.000000      ...           1.000000           0.000000   \n",
      "max       65.000000      ...           1.000000           5.000000   \n",
      "\n",
      "       waiting_4_loan  cancelled_loan  received_loan  rejected_loan  \\\n",
      "count    26996.000000    26996.000000   26996.000000   26996.000000   \n",
      "mean         0.001296        0.018818       0.018188       0.004890   \n",
      "std          0.035984        0.135883       0.133633       0.069756   \n",
      "min          0.000000        0.000000       0.000000       0.000000   \n",
      "25%          0.000000        0.000000       0.000000       0.000000   \n",
      "50%          0.000000        0.000000       0.000000       0.000000   \n",
      "75%          0.000000        0.000000       0.000000       0.000000   \n",
      "max          1.000000        1.000000       1.000000       1.000000   \n",
      "\n",
      "       left_for_two_month_plus  left_for_one_month   reward_rate   is_referred  \n",
      "count              26996.00000        26996.000000  26996.000000  26996.000000  \n",
      "mean                   0.17347            0.018077      0.907819      0.318010  \n",
      "std                    0.37866            0.133232      0.751991      0.465712  \n",
      "min                    0.00000            0.000000      0.000000      0.000000  \n",
      "25%                    0.00000            0.000000      0.200000      0.000000  \n",
      "50%                    0.00000            0.000000      0.780000      0.000000  \n",
      "75%                    0.00000            0.000000      1.530000      1.000000  \n",
      "max                    1.00000            1.000000      4.000000      1.000000  \n",
      "\n",
      "[8 rows x 24 columns]\n",
      "Index(['user', 'churn', 'age', 'housing', 'deposits', 'withdrawal',\n",
      "       'purchases_partners', 'purchases', 'cc_taken', 'cc_recommended',\n",
      "       'cc_disliked', 'cc_liked', 'cc_application_begin', 'app_downloaded',\n",
      "       'web_user', 'android_user', 'registered_phones', 'payment_type',\n",
      "       'waiting_4_loan', 'cancelled_loan', 'received_loan', 'rejected_loan',\n",
      "       'zodiac_sign', 'left_for_two_month_plus', 'left_for_one_month',\n",
      "       'reward_rate', 'is_referred'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('new_churn_data.csv')\n",
    "print(dataset.describe())\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = dataset['user']\n",
    "dataset = dataset.drop(columns = ['user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['churn', 'age', 'deposits', 'withdrawal', 'purchases_partners',\n",
       "       'purchases', 'cc_taken', 'cc_recommended', 'cc_disliked', 'cc_liked',\n",
       "       'cc_application_begin', 'app_downloaded', 'web_user', 'android_user',\n",
       "       'registered_phones', 'waiting_4_loan', 'cancelled_loan',\n",
       "       'received_loan', 'rejected_loan', 'left_for_two_month_plus',\n",
       "       'left_for_one_month', 'reward_rate', 'is_referred', 'housing_O',\n",
       "       'housing_R', 'housing_na', 'payment_type_Bi-Weekly',\n",
       "       'payment_type_Monthly', 'payment_type_Semi-Monthly',\n",
       "       'payment_type_Weekly', 'payment_type_na', 'zodiac_sign_Aquarius',\n",
       "       'zodiac_sign_Aries', 'zodiac_sign_Cancer', 'zodiac_sign_Capricorn',\n",
       "       'zodiac_sign_Gemini', 'zodiac_sign_Leo', 'zodiac_sign_Libra',\n",
       "       'zodiac_sign_Pisces', 'zodiac_sign_Sagittarius', 'zodiac_sign_Scorpio',\n",
       "       'zodiac_sign_Taurus', 'zodiac_sign_Virgo', 'zodiac_sign_na'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Hot Encoding: This will introduce new columns for all the categorical columns. Very smart!!\n",
    "dataset = pd.get_dummies(dataset)\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prevent dummy variable trap\n",
    "dataset = dataset.drop(columns = ['housing_na', 'zodiac_sign_na', 'payment_type_na'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.drop(columns = 'churn'), dataset['churn'],\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing churn column distribution:\n",
      "0    12656\n",
      "1     8940\n",
      "Name: churn, dtype: int64\n",
      "After balancing churn column distribution:\n",
      "1    8940\n",
      "0    8940\n",
      "Name: churn, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Balancing the Training Set\n",
    "'''\n",
    "suppose the training set which was prepared is having data with around 60% churn = 0\n",
    "In our case we have data as such.\n",
    "Churn 1 was around 12K+ and churn 0 was around 8K+. This can create a bias\n",
    "Our model will be performing great even though the accuracy is 60%\n",
    "But there is some bias towards churn = 0. So our DS is pretty spread out.\n",
    "But in some case this doesn't happen after train test split.\n",
    "Thus it is needed to perform Training set balancing\n",
    "'''\n",
    "print('Before balancing churn column distribution:\\n' + str(y_train.value_counts()))\n",
    "\n",
    "pos_index = y_train[y_train.values == 1].index\n",
    "neg_index = y_train[y_train.values == 0].index\n",
    "\n",
    "'''\n",
    "Here we are balancing the positive and negative churns in the training set at random\n",
    "'''\n",
    "\n",
    "if len(pos_index) > len(neg_index):\n",
    "    higher = pos_index\n",
    "    lower = neg_index\n",
    "else:\n",
    "    higher = neg_index\n",
    "    lower = pos_index\n",
    "\n",
    "\n",
    "'''\n",
    "Length of churn 0 is more than length of churn 1.\n",
    "Thus we are chopping churn 0 to the length of churn 1\n",
    "'''\n",
    "random.seed(0)\n",
    "higher = np.random.choice(higher, size=len(lower))\n",
    "lower = np.asarray(lower)\n",
    "new_indexes = np.concatenate((lower, higher))\n",
    "\n",
    "X_train = X_train.loc[new_indexes,]\n",
    "y_train = y_train[new_indexes]\n",
    "\n",
    "'''\n",
    "Now churn 1 count = churn 0 count = 8940 nice!!!\n",
    "Run the code below\n",
    "'''\n",
    "print('After balancing churn column distribution:\\n' + str(y_train.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train2 = pd.DataFrame(sc_X.fit_transform(X_train))\n",
    "X_test2 = pd.DataFrame(sc_X.transform(X_test))\n",
    "X_train2.columns = X_train.columns.values\n",
    "X_test2.columns = X_test.columns.values\n",
    "X_train2.index = X_train.index.values\n",
    "X_test2.index = X_test.index.values\n",
    "X_train = X_train2\n",
    "X_test = X_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17880, 40)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=40, units=21, kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=21, kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17880/17880 [==============================] - 8s 426us/step - loss: 0.6355 - acc: 0.6341\n",
      "Epoch 2/100\n",
      "17880/17880 [==============================] - 6s 358us/step - loss: 0.6161 - acc: 0.6599\n",
      "Epoch 3/100\n",
      "17880/17880 [==============================] - 8s 468us/step - loss: 0.6084 - acc: 0.6641\n",
      "Epoch 4/100\n",
      "17880/17880 [==============================] - 6s 339us/step - loss: 0.6022 - acc: 0.6702\n",
      "Epoch 5/100\n",
      "17880/17880 [==============================] - 6s 350us/step - loss: 0.5959 - acc: 0.6767\n",
      "Epoch 6/100\n",
      "17880/17880 [==============================] - 5s 303us/step - loss: 0.5917 - acc: 0.6796\n",
      "Epoch 7/100\n",
      "17880/17880 [==============================] - 6s 319us/step - loss: 0.5881 - acc: 0.6829\n",
      "Epoch 8/100\n",
      "17880/17880 [==============================] - 7s 380us/step - loss: 0.5838 - acc: 0.6846\n",
      "Epoch 9/100\n",
      "17880/17880 [==============================] - 5s 285us/step - loss: 0.5809 - acc: 0.6881\n",
      "Epoch 10/100\n",
      "17880/17880 [==============================] - 5s 275us/step - loss: 0.5773 - acc: 0.6927\n",
      "Epoch 11/100\n",
      "17880/17880 [==============================] - 5s 276us/step - loss: 0.5750 - acc: 0.6924\n",
      "Epoch 12/100\n",
      "17880/17880 [==============================] - 5s 268us/step - loss: 0.5712 - acc: 0.6954\n",
      "Epoch 13/100\n",
      "17880/17880 [==============================] - 4s 241us/step - loss: 0.5691 - acc: 0.6971\n",
      "Epoch 14/100\n",
      "17880/17880 [==============================] - 6s 356us/step - loss: 0.5674 - acc: 0.7033\n",
      "Epoch 15/100\n",
      "17880/17880 [==============================] - 7s 401us/step - loss: 0.5646 - acc: 0.7033 1s -\n",
      "Epoch 16/100\n",
      "17880/17880 [==============================] - 8s 460us/step - loss: 0.5639 - acc: 0.7056\n",
      "Epoch 17/100\n",
      "17880/17880 [==============================] - 8s 434us/step - loss: 0.5621 - acc: 0.7064\n",
      "Epoch 18/100\n",
      "17880/17880 [==============================] - 9s 500us/step - loss: 0.5598 - acc: 0.7072\n",
      "Epoch 19/100\n",
      "17880/17880 [==============================] - 8s 440us/step - loss: 0.5593 - acc: 0.7089\n",
      "Epoch 20/100\n",
      "17880/17880 [==============================] - 9s 490us/step - loss: 0.5574 - acc: 0.7079\n",
      "Epoch 21/100\n",
      "17880/17880 [==============================] - 9s 499us/step - loss: 0.5557 - acc: 0.7076\n",
      "Epoch 22/100\n",
      "17880/17880 [==============================] - 6s 323us/step - loss: 0.5543 - acc: 0.7106\n",
      "Epoch 23/100\n",
      "17880/17880 [==============================] - 6s 331us/step - loss: 0.5536 - acc: 0.7094 0s - loss: 0.5534 - acc: 0.709\n",
      "Epoch 24/100\n",
      "17880/17880 [==============================] - 6s 352us/step - loss: 0.5525 - acc: 0.7130\n",
      "Epoch 25/100\n",
      "17880/17880 [==============================] - 6s 348us/step - loss: 0.5503 - acc: 0.7143\n",
      "Epoch 26/100\n",
      "17880/17880 [==============================] - 6s 349us/step - loss: 0.5489 - acc: 0.7143\n",
      "Epoch 27/100\n",
      "17880/17880 [==============================] - 6s 345us/step - loss: 0.5481 - acc: 0.7162\n",
      "Epoch 28/100\n",
      "17880/17880 [==============================] - 6s 346us/step - loss: 0.5476 - acc: 0.7169\n",
      "Epoch 29/100\n",
      "17880/17880 [==============================] - 6s 337us/step - loss: 0.5469 - acc: 0.7167\n",
      "Epoch 30/100\n",
      "17880/17880 [==============================] - 6s 341us/step - loss: 0.5456 - acc: 0.7172\n",
      "Epoch 31/100\n",
      "17880/17880 [==============================] - 6s 342us/step - loss: 0.5437 - acc: 0.7191\n",
      "Epoch 32/100\n",
      "17880/17880 [==============================] - 6s 341us/step - loss: 0.5442 - acc: 0.7191\n",
      "Epoch 33/100\n",
      "17880/17880 [==============================] - 6s 341us/step - loss: 0.5420 - acc: 0.7206\n",
      "Epoch 34/100\n",
      "17880/17880 [==============================] - 6s 338us/step - loss: 0.5415 - acc: 0.7209\n",
      "Epoch 35/100\n",
      "17880/17880 [==============================] - 6s 334us/step - loss: 0.5402 - acc: 0.7216\n",
      "Epoch 36/100\n",
      "17880/17880 [==============================] - 6s 334us/step - loss: 0.5390 - acc: 0.7234\n",
      "Epoch 37/100\n",
      "17880/17880 [==============================] - 6s 326us/step - loss: 0.5380 - acc: 0.7197\n",
      "Epoch 38/100\n",
      "17880/17880 [==============================] - 6s 330us/step - loss: 0.5369 - acc: 0.7228\n",
      "Epoch 39/100\n",
      "17880/17880 [==============================] - 6s 325us/step - loss: 0.5362 - acc: 0.7213\n",
      "Epoch 40/100\n",
      "17880/17880 [==============================] - 6s 327us/step - loss: 0.5354 - acc: 0.7242\n",
      "Epoch 41/100\n",
      "17880/17880 [==============================] - 6s 324us/step - loss: 0.5352 - acc: 0.7223\n",
      "Epoch 42/100\n",
      "17880/17880 [==============================] - 6s 323us/step - loss: 0.5327 - acc: 0.7275\n",
      "Epoch 43/100\n",
      "17880/17880 [==============================] - 6s 347us/step - loss: 0.5330 - acc: 0.7254\n",
      "Epoch 44/100\n",
      "17880/17880 [==============================] - 6s 351us/step - loss: 0.5322 - acc: 0.7287\n",
      "Epoch 45/100\n",
      "17880/17880 [==============================] - 6s 332us/step - loss: 0.5308 - acc: 0.7268\n",
      "Epoch 46/100\n",
      "17880/17880 [==============================] - 6s 317us/step - loss: 0.5325 - acc: 0.7265\n",
      "Epoch 47/100\n",
      "17880/17880 [==============================] - 5s 299us/step - loss: 0.5304 - acc: 0.7288\n",
      "Epoch 48/100\n",
      "17880/17880 [==============================] - 5s 300us/step - loss: 0.5303 - acc: 0.7285\n",
      "Epoch 49/100\n",
      "17880/17880 [==============================] - 5s 298us/step - loss: 0.5295 - acc: 0.7290\n",
      "Epoch 50/100\n",
      "17880/17880 [==============================] - 5s 295us/step - loss: 0.5290 - acc: 0.7283\n",
      "Epoch 51/100\n",
      "17880/17880 [==============================] - 5s 299us/step - loss: 0.5286 - acc: 0.7284\n",
      "Epoch 52/100\n",
      "17880/17880 [==============================] - 6s 309us/step - loss: 0.5284 - acc: 0.7295\n",
      "Epoch 53/100\n",
      "17880/17880 [==============================] - 5s 300us/step - loss: 0.5265 - acc: 0.7304\n",
      "Epoch 54/100\n",
      "17880/17880 [==============================] - 5s 296us/step - loss: 0.5264 - acc: 0.7286\n",
      "Epoch 55/100\n",
      "17880/17880 [==============================] - 5s 295us/step - loss: 0.5247 - acc: 0.7307\n",
      "Epoch 56/100\n",
      "17880/17880 [==============================] - 5s 294us/step - loss: 0.5254 - acc: 0.7303\n",
      "Epoch 57/100\n",
      "17880/17880 [==============================] - 5s 292us/step - loss: 0.5241 - acc: 0.7341\n",
      "Epoch 58/100\n",
      "17880/17880 [==============================] - 5s 290us/step - loss: 0.5253 - acc: 0.7281\n",
      "Epoch 59/100\n",
      "17880/17880 [==============================] - 6s 314us/step - loss: 0.5222 - acc: 0.7324\n",
      "Epoch 60/100\n",
      "17880/17880 [==============================] - 5s 299us/step - loss: 0.5236 - acc: 0.7303\n",
      "Epoch 61/100\n",
      "17880/17880 [==============================] - 5s 298us/step - loss: 0.5240 - acc: 0.7318\n",
      "Epoch 62/100\n",
      "17880/17880 [==============================] - 5s 291us/step - loss: 0.5222 - acc: 0.7323\n",
      "Epoch 63/100\n",
      "17880/17880 [==============================] - 5s 291us/step - loss: 0.5216 - acc: 0.7329\n",
      "Epoch 64/100\n",
      "17880/17880 [==============================] - 5s 289us/step - loss: 0.5214 - acc: 0.7331\n",
      "Epoch 65/100\n",
      "17880/17880 [==============================] - 5s 287us/step - loss: 0.5211 - acc: 0.7317\n",
      "Epoch 66/100\n",
      "17880/17880 [==============================] - 5s 287us/step - loss: 0.5210 - acc: 0.7349\n",
      "Epoch 67/100\n",
      "17880/17880 [==============================] - 5s 286us/step - loss: 0.5195 - acc: 0.7337\n",
      "Epoch 68/100\n",
      "17880/17880 [==============================] - 5s 286us/step - loss: 0.5207 - acc: 0.7329\n",
      "Epoch 69/100\n",
      "17880/17880 [==============================] - 5s 290us/step - loss: 0.5194 - acc: 0.7344\n",
      "Epoch 70/100\n",
      "17880/17880 [==============================] - 5s 285us/step - loss: 0.5192 - acc: 0.7347\n",
      "Epoch 71/100\n",
      "17880/17880 [==============================] - 5s 283us/step - loss: 0.5182 - acc: 0.7342\n",
      "Epoch 72/100\n",
      "17880/17880 [==============================] - 5s 281us/step - loss: 0.5176 - acc: 0.7353\n",
      "Epoch 73/100\n",
      "17880/17880 [==============================] - 5s 283us/step - loss: 0.5176 - acc: 0.7380\n",
      "Epoch 74/100\n",
      "17880/17880 [==============================] - 5s 284us/step - loss: 0.5171 - acc: 0.7359\n",
      "Epoch 75/100\n",
      "17880/17880 [==============================] - 5s 283us/step - loss: 0.5174 - acc: 0.7346\n",
      "Epoch 76/100\n",
      "17880/17880 [==============================] - 5s 281us/step - loss: 0.5165 - acc: 0.7375\n",
      "Epoch 77/100\n",
      "17880/17880 [==============================] - 5s 281us/step - loss: 0.5170 - acc: 0.7379\n",
      "Epoch 78/100\n",
      "17880/17880 [==============================] - 5s 286us/step - loss: 0.5162 - acc: 0.7398\n",
      "Epoch 79/100\n",
      "17880/17880 [==============================] - 5s 280us/step - loss: 0.5148 - acc: 0.7355\n",
      "Epoch 80/100\n",
      "17880/17880 [==============================] - 5s 286us/step - loss: 0.5157 - acc: 0.7391\n",
      "Epoch 81/100\n",
      "17880/17880 [==============================] - 5s 285us/step - loss: 0.5156 - acc: 0.7365\n",
      "Epoch 82/100\n",
      "17880/17880 [==============================] - 5s 283us/step - loss: 0.5133 - acc: 0.7407\n",
      "Epoch 83/100\n",
      "17880/17880 [==============================] - 5s 279us/step - loss: 0.5141 - acc: 0.7393\n",
      "Epoch 84/100\n",
      "17880/17880 [==============================] - 5s 280us/step - loss: 0.5138 - acc: 0.7375\n",
      "Epoch 85/100\n",
      "17880/17880 [==============================] - 5s 277us/step - loss: 0.5134 - acc: 0.7399\n",
      "Epoch 86/100\n",
      "17880/17880 [==============================] - 5s 279us/step - loss: 0.5123 - acc: 0.7404\n",
      "Epoch 87/100\n",
      "17880/17880 [==============================] - 5s 277us/step - loss: 0.5125 - acc: 0.7416\n",
      "Epoch 88/100\n",
      "17880/17880 [==============================] - 5s 275us/step - loss: 0.5121 - acc: 0.7405\n",
      "Epoch 89/100\n",
      "17880/17880 [==============================] - 5s 286us/step - loss: 0.5122 - acc: 0.7406\n",
      "Epoch 90/100\n",
      "17880/17880 [==============================] - 5s 275us/step - loss: 0.5121 - acc: 0.7435\n",
      "Epoch 91/100\n",
      "17880/17880 [==============================] - 5s 277us/step - loss: 0.5117 - acc: 0.7416\n",
      "Epoch 92/100\n",
      "17880/17880 [==============================] - 5s 275us/step - loss: 0.5117 - acc: 0.7407\n",
      "Epoch 93/100\n",
      "17880/17880 [==============================] - 5s 274us/step - loss: 0.5105 - acc: 0.7394\n",
      "Epoch 94/100\n",
      "17880/17880 [==============================] - 5s 274us/step - loss: 0.5111 - acc: 0.7388\n",
      "Epoch 95/100\n",
      "17880/17880 [==============================] - 5s 274us/step - loss: 0.5100 - acc: 0.7405\n",
      "Epoch 96/100\n",
      "17880/17880 [==============================] - 5s 272us/step - loss: 0.5105 - acc: 0.7433\n",
      "Epoch 97/100\n",
      "17880/17880 [==============================] - 5s 271us/step - loss: 0.5091 - acc: 0.7410\n",
      "Epoch 98/100\n",
      "17880/17880 [==============================] - 5s 271us/step - loss: 0.5089 - acc: 0.7403\n",
      "Epoch 99/100\n",
      "17880/17880 [==============================] - 5s 269us/step - loss: 0.5079 - acc: 0.7424\n",
      "Epoch 100/100\n",
      "17880/17880 [==============================] - 5s 271us/step - loss: 0.5082 - acc: 0.7447\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 21, init = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 21, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n",
    "classifier.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "cnn_model = load_model('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2834      True\n",
       "21018    False\n",
       "5590     False\n",
       "15769    False\n",
       "15120    False\n",
       "17510    False\n",
       "21136     True\n",
       "7994      True\n",
       "2924      True\n",
       "26906    False\n",
       "14574    False\n",
       "15038    False\n",
       "25891     True\n",
       "2198     False\n",
       "16766     True\n",
       "17116     True\n",
       "6220     False\n",
       "19173    False\n",
       "17473     True\n",
       "16945    False\n",
       "19321    False\n",
       "23173    False\n",
       "8046      True\n",
       "25979     True\n",
       "22425    False\n",
       "1834      True\n",
       "24661    False\n",
       "7763      True\n",
       "19537    False\n",
       "12157     True\n",
       "         ...  \n",
       "1470     False\n",
       "25778     True\n",
       "16140    False\n",
       "10589     True\n",
       "11615    False\n",
       "26766     True\n",
       "19538     True\n",
       "18317    False\n",
       "20011     True\n",
       "19300    False\n",
       "21828     True\n",
       "24591    False\n",
       "6611     False\n",
       "26386    False\n",
       "8728     False\n",
       "16706     True\n",
       "3042      True\n",
       "22462    False\n",
       "24622    False\n",
       "18678     True\n",
       "2719     False\n",
       "6518      True\n",
       "22461    False\n",
       "14486    False\n",
       "22169    False\n",
       "16090     True\n",
       "9851     False\n",
       "654       True\n",
       "26248     True\n",
       "10769    False\n",
       "Name: churn, Length: 5400, dtype: bool"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = cnn_model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "y_test = (y_test == 1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[2044 1122]\n",
      " [ 782 1452]]\n",
      "accuracy_score:0.6474074074074074\n",
      "precision_score:0.5641025641025641\n",
      "recall_score:0.6499552372426142\n",
      "f1_score:0.6039933444259568\n"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "# Evaluating Results\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion matrix:\\n' + str(cm))\n",
    "print('accuracy_score:' + str(accuracy_score(y_test, y_pred)))\n",
    "print('precision_score:' + str(precision_score(y_test, y_pred))) # tp / (tp + fp)\n",
    "print('recall_score:' + str(recall_score(y_test, y_pred))) # tp / (tp + fn)\n",
    "print('f1_score:' + str(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Performing k fold cross validation on our deep learning model\n",
    "Now k fold cross validation is in  sklearn and keras is in keras.\n",
    "There is a way to combine these two which is called keras wrappers for scikitlearn.\n",
    "1st: define a method which is a definition of our custom classifier\n",
    "2nd: perform wrapping of keras and sklearn\n",
    "3rd: perform cross_val_score for k fold cross validation.\n",
    "Make sure to give argument n_jobs = -1 as DL models takes many iterations to perform k fold cross validation due to presence of epoch\n",
    "This argument employs all the CPUs to make the k fold cross val\n",
    "'''\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(output_dim = 21, init = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n",
    "    classifier.add(Dense(output_dim = 21, init = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=40, units=21, kernel_initializer=\"uniform\")`\n",
      "  app.launch_new_instance()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=21, kernel_initializer=\"uniform\")`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16092/16092 [==============================] - 7s 441us/step - loss: 0.6371 - acc: 0.6276\n",
      "Epoch 2/100\n",
      "16092/16092 [==============================] - 5s 310us/step - loss: 0.6186 - acc: 0.6541\n",
      "Epoch 3/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.6096 - acc: 0.6648\n",
      "Epoch 4/100\n",
      "16092/16092 [==============================] - 5s 310us/step - loss: 0.6014 - acc: 0.6731\n",
      "Epoch 5/100\n",
      "16092/16092 [==============================] - 5s 299us/step - loss: 0.5976 - acc: 0.6754\n",
      "Epoch 6/100\n",
      "16092/16092 [==============================] - 5s 322us/step - loss: 0.5914 - acc: 0.6810\n",
      "Epoch 7/100\n",
      "16092/16092 [==============================] - 8s 488us/step - loss: 0.5865 - acc: 0.6866\n",
      "Epoch 8/100\n",
      "16092/16092 [==============================] - 8s 474us/step - loss: 0.5824 - acc: 0.6895\n",
      "Epoch 9/100\n",
      "16092/16092 [==============================] - 8s 469us/step - loss: 0.5774 - acc: 0.6946\n",
      "Epoch 10/100\n",
      "16092/16092 [==============================] - 8s 503us/step - loss: 0.5730 - acc: 0.6957\n",
      "Epoch 11/100\n",
      "16092/16092 [==============================] - 8s 500us/step - loss: 0.5690 - acc: 0.7017\n",
      "Epoch 12/100\n",
      "16092/16092 [==============================] - 7s 452us/step - loss: 0.5671 - acc: 0.7017\n",
      "Epoch 13/100\n",
      "16092/16092 [==============================] - 8s 492us/step - loss: 0.5640 - acc: 0.7066\n",
      "Epoch 14/100\n",
      "16092/16092 [==============================] - 8s 473us/step - loss: 0.5617 - acc: 0.7057\n",
      "Epoch 15/100\n",
      "16092/16092 [==============================] - 7s 461us/step - loss: 0.5595 - acc: 0.7115\n",
      "Epoch 16/100\n",
      "16092/16092 [==============================] - 8s 472us/step - loss: 0.5574 - acc: 0.7105\n",
      "Epoch 17/100\n",
      "16092/16092 [==============================] - 8s 480us/step - loss: 0.5554 - acc: 0.7126\n",
      "Epoch 18/100\n",
      "16092/16092 [==============================] - 8s 467us/step - loss: 0.5539 - acc: 0.7148 0s - loss: 0.5\n",
      "Epoch 19/100\n",
      "16092/16092 [==============================] - 7s 436us/step - loss: 0.5526 - acc: 0.7182\n",
      "Epoch 20/100\n",
      "16092/16092 [==============================] - 7s 460us/step - loss: 0.5518 - acc: 0.7177\n",
      "Epoch 21/100\n",
      "16092/16092 [==============================] - 7s 448us/step - loss: 0.5485 - acc: 0.7182\n",
      "Epoch 22/100\n",
      "16092/16092 [==============================] - 7s 464us/step - loss: 0.5492 - acc: 0.7192\n",
      "Epoch 23/100\n",
      "16092/16092 [==============================] - 7s 464us/step - loss: 0.5468 - acc: 0.7194\n",
      "Epoch 24/100\n",
      "16092/16092 [==============================] - 8s 487us/step - loss: 0.5456 - acc: 0.7234\n",
      "Epoch 25/100\n",
      "16092/16092 [==============================] - 7s 459us/step - loss: 0.5447 - acc: 0.7209\n",
      "Epoch 26/100\n",
      "16092/16092 [==============================] - 8s 477us/step - loss: 0.5426 - acc: 0.7232\n",
      "Epoch 27/100\n",
      "16092/16092 [==============================] - 7s 434us/step - loss: 0.5409 - acc: 0.7252\n",
      "Epoch 28/100\n",
      "16092/16092 [==============================] - 5s 283us/step - loss: 0.5416 - acc: 0.7233\n",
      "Epoch 29/100\n",
      "16092/16092 [==============================] - 5s 293us/step - loss: 0.5392 - acc: 0.7241\n",
      "Epoch 30/100\n",
      "16092/16092 [==============================] - 7s 416us/step - loss: 0.5400 - acc: 0.7264\n",
      "Epoch 31/100\n",
      "16092/16092 [==============================] - 6s 352us/step - loss: 0.5375 - acc: 0.7279\n",
      "Epoch 32/100\n",
      "16092/16092 [==============================] - 5s 286us/step - loss: 0.5372 - acc: 0.7254\n",
      "Epoch 33/100\n",
      "16092/16092 [==============================] - 4s 242us/step - loss: 0.5364 - acc: 0.7256\n",
      "Epoch 34/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5350 - acc: 0.7291\n",
      "Epoch 35/100\n",
      "16092/16092 [==============================] - 4s 264us/step - loss: 0.5346 - acc: 0.7290\n",
      "Epoch 36/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5341 - acc: 0.7271\n",
      "Epoch 37/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5328 - acc: 0.7297\n",
      "Epoch 38/100\n",
      "16092/16092 [==============================] - 5s 284us/step - loss: 0.5328 - acc: 0.7297\n",
      "Epoch 39/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5324 - acc: 0.7274\n",
      "Epoch 40/100\n",
      "16092/16092 [==============================] - 4s 255us/step - loss: 0.5310 - acc: 0.7314\n",
      "Epoch 41/100\n",
      "16092/16092 [==============================] - 4s 251us/step - loss: 0.5291 - acc: 0.7314\n",
      "Epoch 42/100\n",
      "16092/16092 [==============================] - 4s 234us/step - loss: 0.5294 - acc: 0.7340\n",
      "Epoch 43/100\n",
      "16092/16092 [==============================] - 4s 234us/step - loss: 0.5285 - acc: 0.7346\n",
      "Epoch 44/100\n",
      "16092/16092 [==============================] - 4s 234us/step - loss: 0.5272 - acc: 0.7337\n",
      "Epoch 45/100\n",
      "16092/16092 [==============================] - 4s 233us/step - loss: 0.5281 - acc: 0.7328\n",
      "Epoch 46/100\n",
      "16092/16092 [==============================] - 4s 233us/step - loss: 0.5260 - acc: 0.7348\n",
      "Epoch 47/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5257 - acc: 0.7350\n",
      "Epoch 48/100\n",
      "16092/16092 [==============================] - 4s 255us/step - loss: 0.5251 - acc: 0.7348\n",
      "Epoch 49/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5243 - acc: 0.7363\n",
      "Epoch 50/100\n",
      "16092/16092 [==============================] - 4s 240us/step - loss: 0.5236 - acc: 0.7334\n",
      "Epoch 51/100\n",
      "16092/16092 [==============================] - 4s 247us/step - loss: 0.5227 - acc: 0.7378\n",
      "Epoch 52/100\n",
      "16092/16092 [==============================] - 4s 249us/step - loss: 0.5235 - acc: 0.7365 1s -\n",
      "Epoch 53/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5216 - acc: 0.7360\n",
      "Epoch 54/100\n",
      "16092/16092 [==============================] - 4s 269us/step - loss: 0.5205 - acc: 0.7398\n",
      "Epoch 55/100\n",
      "16092/16092 [==============================] - 4s 256us/step - loss: 0.5195 - acc: 0.7374\n",
      "Epoch 56/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5198 - acc: 0.7387\n",
      "Epoch 57/100\n",
      "16092/16092 [==============================] - 4s 270us/step - loss: 0.5192 - acc: 0.7373\n",
      "Epoch 58/100\n",
      "16092/16092 [==============================] - 5s 280us/step - loss: 0.5180 - acc: 0.7384\n",
      "Epoch 59/100\n",
      "16092/16092 [==============================] - 4s 280us/step - loss: 0.5176 - acc: 0.7401\n",
      "Epoch 60/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5163 - acc: 0.7415\n",
      "Epoch 61/100\n",
      "16092/16092 [==============================] - 4s 262us/step - loss: 0.5168 - acc: 0.7414\n",
      "Epoch 62/100\n",
      "16092/16092 [==============================] - 4s 246us/step - loss: 0.5159 - acc: 0.7429\n",
      "Epoch 63/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5157 - acc: 0.7406\n",
      "Epoch 64/100\n",
      "16092/16092 [==============================] - 4s 234us/step - loss: 0.5143 - acc: 0.7434\n",
      "Epoch 65/100\n",
      "16092/16092 [==============================] - 4s 250us/step - loss: 0.5136 - acc: 0.7432\n",
      "Epoch 66/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5144 - acc: 0.7443\n",
      "Epoch 67/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5132 - acc: 0.7435\n",
      "Epoch 68/100\n",
      "16092/16092 [==============================] - 4s 235us/step - loss: 0.5128 - acc: 0.7441\n",
      "Epoch 69/100\n",
      "16092/16092 [==============================] - 4s 246us/step - loss: 0.5128 - acc: 0.7417\n",
      "Epoch 70/100\n",
      "16092/16092 [==============================] - 4s 249us/step - loss: 0.5115 - acc: 0.7430\n",
      "Epoch 71/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5116 - acc: 0.7415\n",
      "Epoch 72/100\n",
      "16092/16092 [==============================] - 4s 252us/step - loss: 0.5112 - acc: 0.7425\n",
      "Epoch 73/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5101 - acc: 0.7457\n",
      "Epoch 74/100\n",
      "16092/16092 [==============================] - 4s 235us/step - loss: 0.5090 - acc: 0.7461\n",
      "Epoch 75/100\n",
      "16092/16092 [==============================] - 4s 231us/step - loss: 0.5094 - acc: 0.7470\n",
      "Epoch 76/100\n",
      "16092/16092 [==============================] - 4s 262us/step - loss: 0.5084 - acc: 0.7449\n",
      "Epoch 77/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5074 - acc: 0.7454\n",
      "Epoch 78/100\n",
      "16092/16092 [==============================] - 4s 232us/step - loss: 0.5070 - acc: 0.7474\n",
      "Epoch 79/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5068 - acc: 0.7443\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 6s 351us/step - loss: 0.5065 - acc: 0.7498\n",
      "Epoch 81/100\n",
      "16092/16092 [==============================] - 6s 401us/step - loss: 0.5058 - acc: 0.7470\n",
      "Epoch 82/100\n",
      "16092/16092 [==============================] - 8s 477us/step - loss: 0.5051 - acc: 0.7478\n",
      "Epoch 83/100\n",
      "16092/16092 [==============================] - 6s 347us/step - loss: 0.5054 - acc: 0.7466\n",
      "Epoch 84/100\n",
      "16092/16092 [==============================] - 5s 326us/step - loss: 0.5040 - acc: 0.7499\n",
      "Epoch 85/100\n",
      "16092/16092 [==============================] - 7s 427us/step - loss: 0.5035 - acc: 0.7496 0s - loss: 0.5031 - acc: 0\n",
      "Epoch 86/100\n",
      "16092/16092 [==============================] - 7s 406us/step - loss: 0.5040 - acc: 0.7491\n",
      "Epoch 87/100\n",
      "16092/16092 [==============================] - 4s 263us/step - loss: 0.5016 - acc: 0.7516\n",
      "Epoch 88/100\n",
      "16092/16092 [==============================] - 4s 278us/step - loss: 0.5036 - acc: 0.7501\n",
      "Epoch 89/100\n",
      "16092/16092 [==============================] - 4s 230us/step - loss: 0.5026 - acc: 0.7498\n",
      "Epoch 90/100\n",
      "16092/16092 [==============================] - 4s 268us/step - loss: 0.5030 - acc: 0.7510\n",
      "Epoch 91/100\n",
      "16092/16092 [==============================] - 5s 297us/step - loss: 0.5018 - acc: 0.7493\n",
      "Epoch 92/100\n",
      "16092/16092 [==============================] - 5s 280us/step - loss: 0.5015 - acc: 0.7521\n",
      "Epoch 93/100\n",
      "16092/16092 [==============================] - 4s 246us/step - loss: 0.5009 - acc: 0.7532\n",
      "Epoch 94/100\n",
      "16092/16092 [==============================] - 6s 402us/step - loss: 0.5006 - acc: 0.7514 1s - los\n",
      "Epoch 95/100\n",
      "16092/16092 [==============================] - 6s 398us/step - loss: 0.5013 - acc: 0.7517 1s - lo\n",
      "Epoch 96/100\n",
      "16092/16092 [==============================] - 7s 447us/step - loss: 0.5005 - acc: 0.7510\n",
      "Epoch 97/100\n",
      "16092/16092 [==============================] - 8s 496us/step - loss: 0.4998 - acc: 0.7514\n",
      "Epoch 98/100\n",
      "16092/16092 [==============================] - 9s 541us/step - loss: 0.4996 - acc: 0.7488\n",
      "Epoch 99/100\n",
      "16092/16092 [==============================] - 8s 474us/step - loss: 0.4994 - acc: 0.7523 1s - loss: 0.5\n",
      "Epoch 100/100\n",
      "16092/16092 [==============================] - 8s 525us/step - loss: 0.4982 - acc: 0.7553\n",
      "1788/1788 [==============================] - 1s 308us/step\n",
      "Epoch 1/100\n",
      "16092/16092 [==============================] - 8s 469us/step - loss: 0.6352 - acc: 0.6252\n",
      "Epoch 2/100\n",
      "16092/16092 [==============================] - 5s 285us/step - loss: 0.6166 - acc: 0.6575\n",
      "Epoch 3/100\n",
      "16092/16092 [==============================] - 4s 255us/step - loss: 0.6075 - acc: 0.6664\n",
      "Epoch 4/100\n",
      "16092/16092 [==============================] - 5s 326us/step - loss: 0.6022 - acc: 0.6713\n",
      "Epoch 5/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5976 - acc: 0.6799\n",
      "Epoch 6/100\n",
      "16092/16092 [==============================] - 4s 273us/step - loss: 0.5924 - acc: 0.6815\n",
      "Epoch 7/100\n",
      "16092/16092 [==============================] - 5s 316us/step - loss: 0.5875 - acc: 0.6870\n",
      "Epoch 8/100\n",
      "16092/16092 [==============================] - 5s 311us/step - loss: 0.5839 - acc: 0.6870\n",
      "Epoch 9/100\n",
      "16092/16092 [==============================] - 5s 288us/step - loss: 0.5808 - acc: 0.6911\n",
      "Epoch 10/100\n",
      "16092/16092 [==============================] - 4s 255us/step - loss: 0.5779 - acc: 0.6926\n",
      "Epoch 11/100\n",
      "16092/16092 [==============================] - 5s 286us/step - loss: 0.5745 - acc: 0.6972\n",
      "Epoch 12/100\n",
      "16092/16092 [==============================] - 5s 318us/step - loss: 0.5720 - acc: 0.6974\n",
      "Epoch 13/100\n",
      "16092/16092 [==============================] - 5s 323us/step - loss: 0.5704 - acc: 0.7002\n",
      "Epoch 14/100\n",
      "16092/16092 [==============================] - 5s 281us/step - loss: 0.5669 - acc: 0.7043\n",
      "Epoch 15/100\n",
      "16092/16092 [==============================] - 4s 235us/step - loss: 0.5653 - acc: 0.7053\n",
      "Epoch 16/100\n",
      "16092/16092 [==============================] - 5s 287us/step - loss: 0.5633 - acc: 0.7066\n",
      "Epoch 17/100\n",
      "16092/16092 [==============================] - 5s 305us/step - loss: 0.5613 - acc: 0.7079\n",
      "Epoch 18/100\n",
      "16092/16092 [==============================] - 5s 322us/step - loss: 0.5592 - acc: 0.7072\n",
      "Epoch 19/100\n",
      "16092/16092 [==============================] - 5s 305us/step - loss: 0.5584 - acc: 0.7102\n",
      "Epoch 20/100\n",
      "16092/16092 [==============================] - 5s 308us/step - loss: 0.5563 - acc: 0.7115\n",
      "Epoch 21/100\n",
      "16092/16092 [==============================] - 5s 324us/step - loss: 0.5549 - acc: 0.7144\n",
      "Epoch 22/100\n",
      "16092/16092 [==============================] - 5s 335us/step - loss: 0.5533 - acc: 0.7112\n",
      "Epoch 23/100\n",
      "16092/16092 [==============================] - 5s 315us/step - loss: 0.5517 - acc: 0.7143\n",
      "Epoch 24/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5499 - acc: 0.7157\n",
      "Epoch 25/100\n",
      "16092/16092 [==============================] - 4s 263us/step - loss: 0.5494 - acc: 0.7165\n",
      "Epoch 26/100\n",
      "16092/16092 [==============================] - 4s 242us/step - loss: 0.5474 - acc: 0.7171\n",
      "Epoch 27/100\n",
      "16092/16092 [==============================] - 4s 236us/step - loss: 0.5451 - acc: 0.7207\n",
      "Epoch 28/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5447 - acc: 0.7186\n",
      "Epoch 29/100\n",
      "16092/16092 [==============================] - 4s 265us/step - loss: 0.5433 - acc: 0.7202\n",
      "Epoch 30/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5413 - acc: 0.7220\n",
      "Epoch 31/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5405 - acc: 0.7181\n",
      "Epoch 32/100\n",
      "16092/16092 [==============================] - 4s 261us/step - loss: 0.5390 - acc: 0.7213\n",
      "Epoch 33/100\n",
      "16092/16092 [==============================] - 4s 273us/step - loss: 0.5394 - acc: 0.7228\n",
      "Epoch 34/100\n",
      "16092/16092 [==============================] - 4s 265us/step - loss: 0.5371 - acc: 0.7206\n",
      "Epoch 35/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5375 - acc: 0.7228\n",
      "Epoch 36/100\n",
      "16092/16092 [==============================] - 4s 253us/step - loss: 0.5359 - acc: 0.7222\n",
      "Epoch 37/100\n",
      "16092/16092 [==============================] - 4s 255us/step - loss: 0.5345 - acc: 0.7247\n",
      "Epoch 38/100\n",
      "16092/16092 [==============================] - 4s 269us/step - loss: 0.5330 - acc: 0.7234\n",
      "Epoch 39/100\n",
      "16092/16092 [==============================] - 4s 262us/step - loss: 0.5332 - acc: 0.7269\n",
      "Epoch 40/100\n",
      "16092/16092 [==============================] - 4s 254us/step - loss: 0.5318 - acc: 0.7257\n",
      "Epoch 41/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5319 - acc: 0.7273\n",
      "Epoch 42/100\n",
      "16092/16092 [==============================] - 6s 359us/step - loss: 0.5304 - acc: 0.7314\n",
      "Epoch 43/100\n",
      "16092/16092 [==============================] - 6s 399us/step - loss: 0.5291 - acc: 0.7281\n",
      "Epoch 44/100\n",
      "16092/16092 [==============================] - 7s 450us/step - loss: 0.5293 - acc: 0.7327\n",
      "Epoch 45/100\n",
      "16092/16092 [==============================] - 8s 473us/step - loss: 0.5276 - acc: 0.7297\n",
      "Epoch 46/100\n",
      "16092/16092 [==============================] - 8s 489us/step - loss: 0.5288 - acc: 0.7297\n",
      "Epoch 47/100\n",
      "16092/16092 [==============================] - 8s 519us/step - loss: 0.5266 - acc: 0.7322\n",
      "Epoch 48/100\n",
      "16092/16092 [==============================] - 7s 458us/step - loss: 0.5258 - acc: 0.7340\n",
      "Epoch 49/100\n",
      "16092/16092 [==============================] - 8s 487us/step - loss: 0.5256 - acc: 0.7329\n",
      "Epoch 50/100\n",
      "16092/16092 [==============================] - 7s 433us/step - loss: 0.5249 - acc: 0.7316\n",
      "Epoch 51/100\n",
      "16092/16092 [==============================] - 4s 260us/step - loss: 0.5247 - acc: 0.7336\n",
      "Epoch 52/100\n",
      "16092/16092 [==============================] - 4s 258us/step - loss: 0.5231 - acc: 0.7366\n",
      "Epoch 53/100\n",
      "16092/16092 [==============================] - 4s 248us/step - loss: 0.5224 - acc: 0.7351\n",
      "Epoch 54/100\n",
      "16092/16092 [==============================] - 4s 252us/step - loss: 0.5235 - acc: 0.7341\n",
      "Epoch 55/100\n",
      "16092/16092 [==============================] - 4s 260us/step - loss: 0.5211 - acc: 0.7351\n",
      "Epoch 56/100\n",
      "16092/16092 [==============================] - 5s 285us/step - loss: 0.5217 - acc: 0.7350\n",
      "Epoch 57/100\n",
      "16092/16092 [==============================] - 7s 444us/step - loss: 0.5207 - acc: 0.7370\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 8s 505us/step - loss: 0.5192 - acc: 0.7389\n",
      "Epoch 59/100\n",
      "16092/16092 [==============================] - 8s 489us/step - loss: 0.5211 - acc: 0.7374\n",
      "Epoch 60/100\n",
      "16092/16092 [==============================] - 7s 463us/step - loss: 0.5176 - acc: 0.7386\n",
      "Epoch 61/100\n",
      "16092/16092 [==============================] - 8s 483us/step - loss: 0.5187 - acc: 0.7366\n",
      "Epoch 62/100\n",
      "16092/16092 [==============================] - 5s 332us/step - loss: 0.5167 - acc: 0.7404\n",
      "Epoch 63/100\n",
      "16092/16092 [==============================] - 4s 260us/step - loss: 0.5177 - acc: 0.7386\n",
      "Epoch 64/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5159 - acc: 0.7398\n",
      "Epoch 65/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5168 - acc: 0.7415\n",
      "Epoch 66/100\n",
      "16092/16092 [==============================] - 4s 246us/step - loss: 0.5153 - acc: 0.7419\n",
      "Epoch 67/100\n",
      "16092/16092 [==============================] - 4s 240us/step - loss: 0.5155 - acc: 0.7402\n",
      "Epoch 68/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5146 - acc: 0.7402\n",
      "Epoch 69/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5148 - acc: 0.7418\n",
      "Epoch 70/100\n",
      "16092/16092 [==============================] - 4s 240us/step - loss: 0.5133 - acc: 0.7449\n",
      "Epoch 71/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5136 - acc: 0.7397\n",
      "Epoch 72/100\n",
      "16092/16092 [==============================] - 4s 231us/step - loss: 0.5134 - acc: 0.7416\n",
      "Epoch 73/100\n",
      "16092/16092 [==============================] - 5s 292us/step - loss: 0.5124 - acc: 0.7422\n",
      "Epoch 74/100\n",
      "16092/16092 [==============================] - 5s 283us/step - loss: 0.5131 - acc: 0.7410\n",
      "Epoch 75/100\n",
      "16092/16092 [==============================] - 4s 250us/step - loss: 0.5121 - acc: 0.7425\n",
      "Epoch 76/100\n",
      "16092/16092 [==============================] - 4s 248us/step - loss: 0.5119 - acc: 0.7407\n",
      "Epoch 77/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5114 - acc: 0.7400\n",
      "Epoch 78/100\n",
      "16092/16092 [==============================] - 4s 251us/step - loss: 0.5103 - acc: 0.7423\n",
      "Epoch 79/100\n",
      "16092/16092 [==============================] - 4s 256us/step - loss: 0.5096 - acc: 0.7436\n",
      "Epoch 80/100\n",
      "16092/16092 [==============================] - 4s 257us/step - loss: 0.5100 - acc: 0.7425\n",
      "Epoch 81/100\n",
      "16092/16092 [==============================] - 4s 254us/step - loss: 0.5102 - acc: 0.7445\n",
      "Epoch 82/100\n",
      "16092/16092 [==============================] - 4s 254us/step - loss: 0.5083 - acc: 0.7458\n",
      "Epoch 83/100\n",
      "16092/16092 [==============================] - 6s 383us/step - loss: 0.5084 - acc: 0.7437\n",
      "Epoch 84/100\n",
      "16092/16092 [==============================] - 7s 412us/step - loss: 0.5077 - acc: 0.7449\n",
      "Epoch 85/100\n",
      "16092/16092 [==============================] - 7s 437us/step - loss: 0.5076 - acc: 0.7460\n",
      "Epoch 86/100\n",
      "16092/16092 [==============================] - 8s 488us/step - loss: 0.5064 - acc: 0.7448\n",
      "Epoch 87/100\n",
      "16092/16092 [==============================] - 8s 498us/step - loss: 0.5066 - acc: 0.7481\n",
      "Epoch 88/100\n",
      "16092/16092 [==============================] - 8s 497us/step - loss: 0.5059 - acc: 0.7476\n",
      "Epoch 89/100\n",
      "16092/16092 [==============================] - 8s 501us/step - loss: 0.5043 - acc: 0.7462\n",
      "Epoch 90/100\n",
      "16092/16092 [==============================] - 8s 519us/step - loss: 0.5045 - acc: 0.7479\n",
      "Epoch 91/100\n",
      "16092/16092 [==============================] - 8s 496us/step - loss: 0.5033 - acc: 0.7465\n",
      "Epoch 92/100\n",
      "16092/16092 [==============================] - 8s 499us/step - loss: 0.5044 - acc: 0.7485\n",
      "Epoch 93/100\n",
      "16092/16092 [==============================] - 8s 483us/step - loss: 0.5040 - acc: 0.7479\n",
      "Epoch 94/100\n",
      "16092/16092 [==============================] - 8s 481us/step - loss: 0.5030 - acc: 0.7489\n",
      "Epoch 95/100\n",
      "16092/16092 [==============================] - 7s 462us/step - loss: 0.5020 - acc: 0.7511\n",
      "Epoch 96/100\n",
      "16092/16092 [==============================] - 5s 318us/step - loss: 0.5021 - acc: 0.7482\n",
      "Epoch 97/100\n",
      "16092/16092 [==============================] - 5s 322us/step - loss: 0.5011 - acc: 0.7499\n",
      "Epoch 98/100\n",
      "16092/16092 [==============================] - 5s 334us/step - loss: 0.5018 - acc: 0.7495\n",
      "Epoch 99/100\n",
      "16092/16092 [==============================] - 5s 330us/step - loss: 0.5006 - acc: 0.7499\n",
      "Epoch 100/100\n",
      "16092/16092 [==============================] - 5s 323us/step - loss: 0.5002 - acc: 0.7498\n",
      "1788/1788 [==============================] - 0s 211us/step\n",
      "Epoch 1/100\n",
      "16092/16092 [==============================] - 6s 368us/step - loss: 0.6353 - acc: 0.6304\n",
      "Epoch 2/100\n",
      "16092/16092 [==============================] - 5s 319us/step - loss: 0.6135 - acc: 0.6646\n",
      "Epoch 3/100\n",
      "16092/16092 [==============================] - 5s 308us/step - loss: 0.6045 - acc: 0.6723\n",
      "Epoch 4/100\n",
      "16092/16092 [==============================] - 5s 325us/step - loss: 0.5966 - acc: 0.6784\n",
      "Epoch 5/100\n",
      "16092/16092 [==============================] - 5s 326us/step - loss: 0.5922 - acc: 0.6785\n",
      "Epoch 6/100\n",
      "16092/16092 [==============================] - 5s 325us/step - loss: 0.5872 - acc: 0.6841\n",
      "Epoch 7/100\n",
      "16092/16092 [==============================] - 5s 326us/step - loss: 0.5837 - acc: 0.6877\n",
      "Epoch 8/100\n",
      "16092/16092 [==============================] - 5s 324us/step - loss: 0.5788 - acc: 0.6908\n",
      "Epoch 9/100\n",
      "16092/16092 [==============================] - 5s 331us/step - loss: 0.5757 - acc: 0.6923\n",
      "Epoch 10/100\n",
      "16092/16092 [==============================] - 5s 323us/step - loss: 0.5728 - acc: 0.6928\n",
      "Epoch 11/100\n",
      "16092/16092 [==============================] - 6s 358us/step - loss: 0.5692 - acc: 0.6998\n",
      "Epoch 12/100\n",
      "16092/16092 [==============================] - 8s 516us/step - loss: 0.5670 - acc: 0.6988\n",
      "Epoch 13/100\n",
      "16092/16092 [==============================] - 8s 521us/step - loss: 0.5638 - acc: 0.7051\n",
      "Epoch 14/100\n",
      "16092/16092 [==============================] - 9s 570us/step - loss: 0.5621 - acc: 0.7077\n",
      "Epoch 15/100\n",
      "16092/16092 [==============================] - 8s 526us/step - loss: 0.5593 - acc: 0.7123\n",
      "Epoch 16/100\n",
      "16092/16092 [==============================] - 8s 507us/step - loss: 0.5564 - acc: 0.7109\n",
      "Epoch 17/100\n",
      "16092/16092 [==============================] - 8s 522us/step - loss: 0.5547 - acc: 0.7145\n",
      "Epoch 18/100\n",
      "16092/16092 [==============================] - 8s 513us/step - loss: 0.5521 - acc: 0.7177\n",
      "Epoch 19/100\n",
      "16092/16092 [==============================] - 8s 509us/step - loss: 0.5514 - acc: 0.7168\n",
      "Epoch 20/100\n",
      "16092/16092 [==============================] - 8s 523us/step - loss: 0.5502 - acc: 0.7181\n",
      "Epoch 21/100\n",
      "16092/16092 [==============================] - 8s 511us/step - loss: 0.5480 - acc: 0.7212\n",
      "Epoch 22/100\n",
      "16092/16092 [==============================] - 8s 510us/step - loss: 0.5474 - acc: 0.7225\n",
      "Epoch 23/100\n",
      "16092/16092 [==============================] - 8s 498us/step - loss: 0.5434 - acc: 0.7237\n",
      "Epoch 24/100\n",
      "16092/16092 [==============================] - 8s 489us/step - loss: 0.5433 - acc: 0.7228\n",
      "Epoch 25/100\n",
      "16092/16092 [==============================] - 8s 510us/step - loss: 0.5413 - acc: 0.7253\n",
      "Epoch 26/100\n",
      "16092/16092 [==============================] - 8s 506us/step - loss: 0.5400 - acc: 0.7246\n",
      "Epoch 27/100\n",
      "16092/16092 [==============================] - 8s 480us/step - loss: 0.5387 - acc: 0.7262 1s - l\n",
      "Epoch 28/100\n",
      "16092/16092 [==============================] - 5s 299us/step - loss: 0.5373 - acc: 0.7278\n",
      "Epoch 29/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5357 - acc: 0.7283\n",
      "Epoch 30/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5343 - acc: 0.7278\n",
      "Epoch 31/100\n",
      "16092/16092 [==============================] - 5s 289us/step - loss: 0.5338 - acc: 0.7317\n",
      "Epoch 32/100\n",
      "16092/16092 [==============================] - 5s 307us/step - loss: 0.5324 - acc: 0.7316\n",
      "Epoch 33/100\n",
      "16092/16092 [==============================] - 5s 304us/step - loss: 0.5307 - acc: 0.7320\n",
      "Epoch 34/100\n",
      "16092/16092 [==============================] - 5s 287us/step - loss: 0.5291 - acc: 0.7330\n",
      "Epoch 35/100\n",
      "16092/16092 [==============================] - 5s 302us/step - loss: 0.5284 - acc: 0.7332\n",
      "Epoch 36/100\n",
      "16092/16092 [==============================] - 5s 305us/step - loss: 0.5277 - acc: 0.7336\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 5s 317us/step - loss: 0.5260 - acc: 0.7348\n",
      "Epoch 38/100\n",
      "16092/16092 [==============================] - 5s 330us/step - loss: 0.5269 - acc: 0.7333\n",
      "Epoch 39/100\n",
      "16092/16092 [==============================] - 5s 320us/step - loss: 0.5249 - acc: 0.7343\n",
      "Epoch 40/100\n",
      "16092/16092 [==============================] - 5s 316us/step - loss: 0.5232 - acc: 0.7365\n",
      "Epoch 41/100\n",
      "16092/16092 [==============================] - 5s 296us/step - loss: 0.5225 - acc: 0.7373\n",
      "Epoch 42/100\n",
      "16092/16092 [==============================] - 5s 299us/step - loss: 0.5225 - acc: 0.7408\n",
      "Epoch 43/100\n",
      "16092/16092 [==============================] - 5s 296us/step - loss: 0.5205 - acc: 0.7404\n",
      "Epoch 44/100\n",
      "16092/16092 [==============================] - 5s 293us/step - loss: 0.5196 - acc: 0.7414\n",
      "Epoch 45/100\n",
      "16092/16092 [==============================] - 5s 293us/step - loss: 0.5204 - acc: 0.7389\n",
      "Epoch 46/100\n",
      "16092/16092 [==============================] - 5s 292us/step - loss: 0.5189 - acc: 0.7429\n",
      "Epoch 47/100\n",
      "16092/16092 [==============================] - 5s 295us/step - loss: 0.5177 - acc: 0.7424\n",
      "Epoch 48/100\n",
      "16092/16092 [==============================] - 5s 315us/step - loss: 0.5171 - acc: 0.7440\n",
      "Epoch 49/100\n",
      "16092/16092 [==============================] - 6s 343us/step - loss: 0.5156 - acc: 0.7422\n",
      "Epoch 50/100\n",
      "16092/16092 [==============================] - 5s 283us/step - loss: 0.5152 - acc: 0.7442\n",
      "Epoch 51/100\n",
      "16092/16092 [==============================] - 5s 285us/step - loss: 0.5144 - acc: 0.7437\n",
      "Epoch 52/100\n",
      "16092/16092 [==============================] - 4s 278us/step - loss: 0.5147 - acc: 0.7423\n",
      "Epoch 53/100\n",
      "16092/16092 [==============================] - 5s 317us/step - loss: 0.5130 - acc: 0.7446\n",
      "Epoch 54/100\n",
      "16092/16092 [==============================] - 7s 413us/step - loss: 0.5134 - acc: 0.7451\n",
      "Epoch 55/100\n",
      "16092/16092 [==============================] - 6s 379us/step - loss: 0.5118 - acc: 0.7478\n",
      "Epoch 56/100\n",
      "16092/16092 [==============================] - 7s 427us/step - loss: 0.5117 - acc: 0.7457\n",
      "Epoch 57/100\n",
      "16092/16092 [==============================] - 7s 466us/step - loss: 0.5102 - acc: 0.7463\n",
      "Epoch 58/100\n",
      "16092/16092 [==============================] - 7s 438us/step - loss: 0.5099 - acc: 0.7452\n",
      "Epoch 59/100\n",
      "16092/16092 [==============================] - 5s 283us/step - loss: 0.5086 - acc: 0.7484\n",
      "Epoch 60/100\n",
      "16092/16092 [==============================] - 4s 254us/step - loss: 0.5081 - acc: 0.7489\n",
      "Epoch 61/100\n",
      "16092/16092 [==============================] - 4s 254us/step - loss: 0.5083 - acc: 0.7465\n",
      "Epoch 62/100\n",
      "16092/16092 [==============================] - 4s 250us/step - loss: 0.5073 - acc: 0.7483\n",
      "Epoch 63/100\n",
      "16092/16092 [==============================] - 5s 312us/step - loss: 0.5069 - acc: 0.7489\n",
      "Epoch 64/100\n",
      "16092/16092 [==============================] - 5s 299us/step - loss: 0.5052 - acc: 0.7487\n",
      "Epoch 65/100\n",
      "16092/16092 [==============================] - 5s 296us/step - loss: 0.5046 - acc: 0.7527\n",
      "Epoch 66/100\n",
      "16092/16092 [==============================] - 5s 301us/step - loss: 0.5038 - acc: 0.7494\n",
      "Epoch 67/100\n",
      "16092/16092 [==============================] - 5s 305us/step - loss: 0.5033 - acc: 0.7498\n",
      "Epoch 68/100\n",
      "16092/16092 [==============================] - 5s 295us/step - loss: 0.5032 - acc: 0.7502\n",
      "Epoch 69/100\n",
      "16092/16092 [==============================] - 5s 302us/step - loss: 0.5032 - acc: 0.7504\n",
      "Epoch 70/100\n",
      "16092/16092 [==============================] - 5s 321us/step - loss: 0.5021 - acc: 0.7511\n",
      "Epoch 71/100\n",
      "16092/16092 [==============================] - 5s 331us/step - loss: 0.5012 - acc: 0.7514\n",
      "Epoch 72/100\n",
      "16092/16092 [==============================] - 5s 305us/step - loss: 0.5012 - acc: 0.7512\n",
      "Epoch 73/100\n",
      "16092/16092 [==============================] - 5s 281us/step - loss: 0.5000 - acc: 0.7529\n",
      "Epoch 74/100\n",
      "16092/16092 [==============================] - 5s 317us/step - loss: 0.4991 - acc: 0.7534\n",
      "Epoch 75/100\n",
      "16092/16092 [==============================] - 5s 316us/step - loss: 0.4988 - acc: 0.7537\n",
      "Epoch 76/100\n",
      "16092/16092 [==============================] - 5s 306us/step - loss: 0.4993 - acc: 0.7521\n",
      "Epoch 77/100\n",
      "16092/16092 [==============================] - 5s 314us/step - loss: 0.4982 - acc: 0.7543\n",
      "Epoch 78/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.4973 - acc: 0.7525\n",
      "Epoch 79/100\n",
      "16092/16092 [==============================] - 5s 285us/step - loss: 0.4981 - acc: 0.7533\n",
      "Epoch 80/100\n",
      "16092/16092 [==============================] - 4s 256us/step - loss: 0.4961 - acc: 0.7562\n",
      "Epoch 81/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.4957 - acc: 0.7527\n",
      "Epoch 82/100\n",
      "16092/16092 [==============================] - 4s 246us/step - loss: 0.4961 - acc: 0.7544\n",
      "Epoch 83/100\n",
      "16092/16092 [==============================] - 4s 248us/step - loss: 0.4954 - acc: 0.7588\n",
      "Epoch 84/100\n",
      "16092/16092 [==============================] - 4s 241us/step - loss: 0.4956 - acc: 0.7549\n",
      "Epoch 85/100\n",
      "16092/16092 [==============================] - 4s 241us/step - loss: 0.4948 - acc: 0.7566\n",
      "Epoch 86/100\n",
      "16092/16092 [==============================] - 4s 258us/step - loss: 0.4935 - acc: 0.7563\n",
      "Epoch 87/100\n",
      "16092/16092 [==============================] - 4s 247us/step - loss: 0.4933 - acc: 0.7548\n",
      "Epoch 88/100\n",
      "16092/16092 [==============================] - 4s 259us/step - loss: 0.4928 - acc: 0.7572\n",
      "Epoch 89/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.4934 - acc: 0.7568\n",
      "Epoch 90/100\n",
      "16092/16092 [==============================] - 4s 250us/step - loss: 0.4910 - acc: 0.7574\n",
      "Epoch 91/100\n",
      "16092/16092 [==============================] - 4s 250us/step - loss: 0.4914 - acc: 0.7570\n",
      "Epoch 92/100\n",
      "16092/16092 [==============================] - 4s 261us/step - loss: 0.4918 - acc: 0.7583\n",
      "Epoch 93/100\n",
      "16092/16092 [==============================] - 5s 294us/step - loss: 0.4910 - acc: 0.7577\n",
      "Epoch 94/100\n",
      "16092/16092 [==============================] - 4s 255us/step - loss: 0.4913 - acc: 0.7587\n",
      "Epoch 95/100\n",
      "16092/16092 [==============================] - 4s 264us/step - loss: 0.4902 - acc: 0.7599\n",
      "Epoch 96/100\n",
      "16092/16092 [==============================] - 4s 233us/step - loss: 0.4913 - acc: 0.7562\n",
      "Epoch 97/100\n",
      "16092/16092 [==============================] - 4s 234us/step - loss: 0.4908 - acc: 0.7578\n",
      "Epoch 98/100\n",
      "16092/16092 [==============================] - 4s 248us/step - loss: 0.4889 - acc: 0.7611\n",
      "Epoch 99/100\n",
      "16092/16092 [==============================] - 4s 249us/step - loss: 0.4896 - acc: 0.7584\n",
      "Epoch 100/100\n",
      "16092/16092 [==============================] - 5s 284us/step - loss: 0.4896 - acc: 0.7594\n",
      "1788/1788 [==============================] - 0s 176us/step\n",
      "Epoch 1/100\n",
      "16092/16092 [==============================] - 5s 309us/step - loss: 0.6352 - acc: 0.6328\n",
      "Epoch 2/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.6164 - acc: 0.6589\n",
      "Epoch 3/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.6079 - acc: 0.6703\n",
      "Epoch 4/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.6011 - acc: 0.6751\n",
      "Epoch 5/100\n",
      "16092/16092 [==============================] - 4s 241us/step - loss: 0.5968 - acc: 0.6770\n",
      "Epoch 6/100\n",
      "16092/16092 [==============================] - 4s 246us/step - loss: 0.5915 - acc: 0.6853\n",
      "Epoch 7/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5882 - acc: 0.6843\n",
      "Epoch 8/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5837 - acc: 0.6889\n",
      "Epoch 9/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5802 - acc: 0.6910\n",
      "Epoch 10/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5767 - acc: 0.6925\n",
      "Epoch 11/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5733 - acc: 0.6956\n",
      "Epoch 12/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5708 - acc: 0.6985\n",
      "Epoch 13/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5678 - acc: 0.7011\n",
      "Epoch 14/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5640 - acc: 0.7057\n",
      "Epoch 15/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5625 - acc: 0.7043\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5605 - acc: 0.7074\n",
      "Epoch 17/100\n",
      "16092/16092 [==============================] - 4s 240us/step - loss: 0.5580 - acc: 0.7089\n",
      "Epoch 18/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5558 - acc: 0.7066\n",
      "Epoch 19/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5562 - acc: 0.7131\n",
      "Epoch 20/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5530 - acc: 0.7133\n",
      "Epoch 21/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5519 - acc: 0.7127\n",
      "Epoch 22/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5502 - acc: 0.7148\n",
      "Epoch 23/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5486 - acc: 0.7138\n",
      "Epoch 24/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5456 - acc: 0.7209\n",
      "Epoch 25/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5445 - acc: 0.7177\n",
      "Epoch 26/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5447 - acc: 0.7188\n",
      "Epoch 27/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5418 - acc: 0.7197\n",
      "Epoch 28/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5420 - acc: 0.7226\n",
      "Epoch 29/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5397 - acc: 0.7220\n",
      "Epoch 30/100\n",
      "16092/16092 [==============================] - 4s 240us/step - loss: 0.5379 - acc: 0.7258\n",
      "Epoch 31/100\n",
      "16092/16092 [==============================] - 4s 259us/step - loss: 0.5367 - acc: 0.7256\n",
      "Epoch 32/100\n",
      "16092/16092 [==============================] - 4s 242us/step - loss: 0.5368 - acc: 0.7256\n",
      "Epoch 33/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5349 - acc: 0.7258\n",
      "Epoch 34/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5345 - acc: 0.7261\n",
      "Epoch 35/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5335 - acc: 0.7283\n",
      "Epoch 36/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5319 - acc: 0.7302\n",
      "Epoch 37/100\n",
      "16092/16092 [==============================] - 4s 249us/step - loss: 0.5306 - acc: 0.7296\n",
      "Epoch 38/100\n",
      "16092/16092 [==============================] - 5s 299us/step - loss: 0.5306 - acc: 0.7268\n",
      "Epoch 39/100\n",
      "16092/16092 [==============================] - 4s 256us/step - loss: 0.5304 - acc: 0.7278\n",
      "Epoch 40/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5302 - acc: 0.7279\n",
      "Epoch 41/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5288 - acc: 0.7298\n",
      "Epoch 42/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5287 - acc: 0.7284\n",
      "Epoch 43/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5260 - acc: 0.7320\n",
      "Epoch 44/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5271 - acc: 0.7289\n",
      "Epoch 45/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5253 - acc: 0.7295\n",
      "Epoch 46/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5250 - acc: 0.7335\n",
      "Epoch 47/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5243 - acc: 0.7342\n",
      "Epoch 48/100\n",
      "16092/16092 [==============================] - 4s 240us/step - loss: 0.5234 - acc: 0.7318\n",
      "Epoch 49/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5233 - acc: 0.7331\n",
      "Epoch 50/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5228 - acc: 0.7324\n",
      "Epoch 51/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5221 - acc: 0.7321\n",
      "Epoch 52/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5227 - acc: 0.7368\n",
      "Epoch 53/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5209 - acc: 0.7333\n",
      "Epoch 54/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5214 - acc: 0.7344\n",
      "Epoch 55/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5206 - acc: 0.7350\n",
      "Epoch 56/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5198 - acc: 0.7366\n",
      "Epoch 57/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5193 - acc: 0.7339\n",
      "Epoch 58/100\n",
      "16092/16092 [==============================] - 4s 240us/step - loss: 0.5191 - acc: 0.7330\n",
      "Epoch 59/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5191 - acc: 0.7352\n",
      "Epoch 60/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5179 - acc: 0.7372\n",
      "Epoch 61/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5183 - acc: 0.7357\n",
      "Epoch 62/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5175 - acc: 0.7357\n",
      "Epoch 63/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5163 - acc: 0.7365\n",
      "Epoch 64/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5165 - acc: 0.7388\n",
      "Epoch 65/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5153 - acc: 0.7396\n",
      "Epoch 66/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5157 - acc: 0.7389\n",
      "Epoch 67/100\n",
      "16092/16092 [==============================] - 4s 237us/step - loss: 0.5141 - acc: 0.7400\n",
      "Epoch 68/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5152 - acc: 0.7409\n",
      "Epoch 69/100\n",
      "16092/16092 [==============================] - 4s 255us/step - loss: 0.5139 - acc: 0.7403\n",
      "Epoch 70/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5144 - acc: 0.7387\n",
      "Epoch 71/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5136 - acc: 0.7402\n",
      "Epoch 72/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5139 - acc: 0.7386 1s - \n",
      "Epoch 73/100\n",
      "16092/16092 [==============================] - 4s 246us/step - loss: 0.5124 - acc: 0.7417\n",
      "Epoch 74/100\n",
      "16092/16092 [==============================] - 4s 246us/step - loss: 0.5114 - acc: 0.7437\n",
      "Epoch 75/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5128 - acc: 0.7417\n",
      "Epoch 76/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5108 - acc: 0.7429\n",
      "Epoch 77/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5107 - acc: 0.7417\n",
      "Epoch 78/100\n",
      "16092/16092 [==============================] - 4s 240us/step - loss: 0.5105 - acc: 0.7441\n",
      "Epoch 79/100\n",
      "16092/16092 [==============================] - 4s 241us/step - loss: 0.5113 - acc: 0.7419\n",
      "Epoch 80/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5089 - acc: 0.7430\n",
      "Epoch 81/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5093 - acc: 0.7438\n",
      "Epoch 82/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5079 - acc: 0.7431\n",
      "Epoch 83/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5090 - acc: 0.7430\n",
      "Epoch 84/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5085 - acc: 0.7415\n",
      "Epoch 85/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5086 - acc: 0.7440\n",
      "Epoch 86/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5073 - acc: 0.7460\n",
      "Epoch 87/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5079 - acc: 0.7438\n",
      "Epoch 88/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5061 - acc: 0.7468\n",
      "Epoch 89/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5061 - acc: 0.7455\n",
      "Epoch 90/100\n",
      "16092/16092 [==============================] - 4s 239us/step - loss: 0.5048 - acc: 0.7451\n",
      "Epoch 91/100\n",
      "16092/16092 [==============================] - 5s 321us/step - loss: 0.5064 - acc: 0.7484\n",
      "Epoch 92/100\n",
      "16092/16092 [==============================] - 5s 320us/step - loss: 0.5046 - acc: 0.7484\n",
      "Epoch 93/100\n",
      "16092/16092 [==============================] - 5s 334us/step - loss: 0.5044 - acc: 0.7459\n",
      "Epoch 94/100\n",
      "16092/16092 [==============================] - 7s 410us/step - loss: 0.5050 - acc: 0.7466\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 9s 540us/step - loss: 0.5035 - acc: 0.7470\n",
      "Epoch 96/100\n",
      "16092/16092 [==============================] - 6s 389us/step - loss: 0.5033 - acc: 0.7452\n",
      "Epoch 97/100\n",
      "16092/16092 [==============================] - 4s 256us/step - loss: 0.5024 - acc: 0.7495\n",
      "Epoch 98/100\n",
      "16092/16092 [==============================] - 4s 240us/step - loss: 0.5044 - acc: 0.7471\n",
      "Epoch 99/100\n",
      "16092/16092 [==============================] - 4s 241us/step - loss: 0.5030 - acc: 0.7490\n",
      "Epoch 100/100\n",
      "16092/16092 [==============================] - 4s 238us/step - loss: 0.5018 - acc: 0.7483\n",
      "1788/1788 [==============================] - 0s 196us/step\n",
      "Epoch 1/100\n",
      "16092/16092 [==============================] - 5s 309us/step - loss: 0.6364 - acc: 0.6299\n",
      "Epoch 2/100\n",
      "16092/16092 [==============================] - 4s 246us/step - loss: 0.6185 - acc: 0.6582\n",
      "Epoch 3/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.6093 - acc: 0.6666\n",
      "Epoch 4/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.6011 - acc: 0.6772\n",
      "Epoch 5/100\n",
      "16092/16092 [==============================] - 4s 246us/step - loss: 0.5955 - acc: 0.6825\n",
      "Epoch 6/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5896 - acc: 0.6833\n",
      "Epoch 7/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5843 - acc: 0.6889\n",
      "Epoch 8/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5810 - acc: 0.6920\n",
      "Epoch 9/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5778 - acc: 0.6914\n",
      "Epoch 10/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5745 - acc: 0.6956\n",
      "Epoch 11/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5716 - acc: 0.6991\n",
      "Epoch 12/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5678 - acc: 0.7024\n",
      "Epoch 13/100\n",
      "16092/16092 [==============================] - 4s 247us/step - loss: 0.5658 - acc: 0.7043\n",
      "Epoch 14/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5635 - acc: 0.7047\n",
      "Epoch 15/100\n",
      "16092/16092 [==============================] - 4s 247us/step - loss: 0.5610 - acc: 0.7069\n",
      "Epoch 16/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5601 - acc: 0.7064\n",
      "Epoch 17/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5583 - acc: 0.7102\n",
      "Epoch 18/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5569 - acc: 0.7110\n",
      "Epoch 19/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5555 - acc: 0.7132\n",
      "Epoch 20/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5551 - acc: 0.7135\n",
      "Epoch 21/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5541 - acc: 0.7105\n",
      "Epoch 22/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5529 - acc: 0.7140\n",
      "Epoch 23/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5513 - acc: 0.7143\n",
      "Epoch 24/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5510 - acc: 0.7159\n",
      "Epoch 25/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5496 - acc: 0.7177\n",
      "Epoch 26/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5481 - acc: 0.7172\n",
      "Epoch 27/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5471 - acc: 0.7212\n",
      "Epoch 28/100\n",
      "16092/16092 [==============================] - ETA: 0s - loss: 0.5468 - acc: 0.721 - 4s 244us/step - loss: 0.5466 - acc: 0.7218\n",
      "Epoch 29/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5462 - acc: 0.7230\n",
      "Epoch 30/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5450 - acc: 0.7191\n",
      "Epoch 31/100\n",
      "16092/16092 [==============================] - 4s 246us/step - loss: 0.5449 - acc: 0.7212\n",
      "Epoch 32/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5432 - acc: 0.7217\n",
      "Epoch 33/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5426 - acc: 0.7209\n",
      "Epoch 34/100\n",
      "16092/16092 [==============================] - 4s 245us/step - loss: 0.5423 - acc: 0.7237\n",
      "Epoch 35/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5408 - acc: 0.7220\n",
      "Epoch 36/100\n",
      "16092/16092 [==============================] - 4s 248us/step - loss: 0.5403 - acc: 0.7271\n",
      "Epoch 37/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5393 - acc: 0.7250\n",
      "Epoch 38/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5386 - acc: 0.7266\n",
      "Epoch 39/100\n",
      "16092/16092 [==============================] - 4s 244us/step - loss: 0.5377 - acc: 0.7240\n",
      "Epoch 40/100\n",
      "16092/16092 [==============================] - 4s 243us/step - loss: 0.5367 - acc: 0.7311\n",
      "Epoch 41/100\n",
      "16092/16092 [==============================] - 4s 248us/step - loss: 0.5370 - acc: 0.7268\n",
      "Epoch 42/100\n",
      "16092/16092 [==============================] - 5s 307us/step - loss: 0.5362 - acc: 0.7283\n",
      "Epoch 43/100\n",
      "16092/16092 [==============================] - 7s 431us/step - loss: 0.5362 - acc: 0.7266\n",
      "Epoch 44/100\n",
      "16092/16092 [==============================] - 7s 430us/step - loss: 0.5358 - acc: 0.7264\n",
      "Epoch 45/100\n",
      "16092/16092 [==============================] - 8s 518us/step - loss: 0.5344 - acc: 0.7264\n",
      "Epoch 46/100\n",
      "16092/16092 [==============================] - 8s 525us/step - loss: 0.5347 - acc: 0.7249\n",
      "Epoch 47/100\n",
      "16092/16092 [==============================] - 9s 538us/step - loss: 0.5334 - acc: 0.7289\n",
      "Epoch 48/100\n",
      "16092/16092 [==============================] - 6s 383us/step - loss: 0.5326 - acc: 0.7289\n",
      "Epoch 49/100\n",
      "16092/16092 [==============================] - 6s 348us/step - loss: 0.5320 - acc: 0.7289\n",
      "Epoch 50/100\n",
      "16092/16092 [==============================] - 5s 328us/step - loss: 0.5312 - acc: 0.7305\n",
      "Epoch 51/100\n",
      "16092/16092 [==============================] - 5s 313us/step - loss: 0.5308 - acc: 0.7297\n",
      "Epoch 52/100\n",
      "16092/16092 [==============================] - 5s 304us/step - loss: 0.5302 - acc: 0.7279\n",
      "Epoch 53/100\n",
      "16092/16092 [==============================] - 5s 315us/step - loss: 0.5295 - acc: 0.7328\n",
      "Epoch 54/100\n",
      "16092/16092 [==============================] - 5s 301us/step - loss: 0.5296 - acc: 0.7302\n",
      "Epoch 55/100\n",
      "16092/16092 [==============================] - 5s 314us/step - loss: 0.5293 - acc: 0.7309\n",
      "Epoch 56/100\n",
      "16092/16092 [==============================] - 5s 324us/step - loss: 0.5271 - acc: 0.7330\n",
      "Epoch 57/100\n",
      "16092/16092 [==============================] - 5s 316us/step - loss: 0.5279 - acc: 0.7304\n",
      "Epoch 58/100\n",
      "16092/16092 [==============================] - 5s 338us/step - loss: 0.5268 - acc: 0.7320\n",
      "Epoch 59/100\n",
      "16092/16092 [==============================] - 5s 333us/step - loss: 0.5267 - acc: 0.7320\n",
      "Epoch 60/100\n",
      "16092/16092 [==============================] - 5s 312us/step - loss: 0.5264 - acc: 0.7340\n",
      "Epoch 61/100\n",
      "16092/16092 [==============================] - 5s 304us/step - loss: 0.5258 - acc: 0.7343\n",
      "Epoch 62/100\n",
      "16092/16092 [==============================] - 5s 320us/step - loss: 0.5248 - acc: 0.7363\n",
      "Epoch 63/100\n",
      "16092/16092 [==============================] - 5s 324us/step - loss: 0.5244 - acc: 0.7370\n",
      "Epoch 64/100\n",
      "16092/16092 [==============================] - 5s 316us/step - loss: 0.5238 - acc: 0.7366\n",
      "Epoch 65/100\n",
      "16092/16092 [==============================] - 5s 316us/step - loss: 0.5230 - acc: 0.7364\n",
      "Epoch 66/100\n",
      "16092/16092 [==============================] - 5s 328us/step - loss: 0.5242 - acc: 0.7333\n",
      "Epoch 67/100\n",
      "16092/16092 [==============================] - 5s 321us/step - loss: 0.5224 - acc: 0.7363\n",
      "Epoch 68/100\n",
      "16092/16092 [==============================] - 5s 328us/step - loss: 0.5220 - acc: 0.7378\n",
      "Epoch 69/100\n",
      "16092/16092 [==============================] - 5s 332us/step - loss: 0.5214 - acc: 0.7383\n",
      "Epoch 70/100\n",
      "16092/16092 [==============================] - 6s 342us/step - loss: 0.5208 - acc: 0.7385\n",
      "Epoch 71/100\n",
      "16092/16092 [==============================] - 6s 375us/step - loss: 0.5202 - acc: 0.7379\n",
      "Epoch 72/100\n",
      "16092/16092 [==============================] - 8s 471us/step - loss: 0.5204 - acc: 0.7372\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 9s 533us/step - loss: 0.5204 - acc: 0.7388 1s - los\n",
      "Epoch 74/100\n",
      "16092/16092 [==============================] - 9s 567us/step - loss: 0.5191 - acc: 0.7381\n",
      "Epoch 75/100\n",
      "16092/16092 [==============================] - 9s 570us/step - loss: 0.5194 - acc: 0.7391\n",
      "Epoch 76/100\n",
      "16092/16092 [==============================] - 9s 574us/step - loss: 0.5185 - acc: 0.7406\n",
      "Epoch 77/100\n",
      "16092/16092 [==============================] - 8s 511us/step - loss: 0.5180 - acc: 0.7392\n",
      "Epoch 78/100\n",
      "16092/16092 [==============================] - 8s 492us/step - loss: 0.5172 - acc: 0.7392\n",
      "Epoch 79/100\n",
      "16092/16092 [==============================] - 8s 518us/step - loss: 0.5170 - acc: 0.7396\n",
      "Epoch 80/100\n",
      "16092/16092 [==============================] - 8s 524us/step - loss: 0.5155 - acc: 0.7381\n",
      "Epoch 81/100\n",
      "16092/16092 [==============================] - 8s 515us/step - loss: 0.5155 - acc: 0.7415\n",
      "Epoch 82/100\n",
      "16092/16092 [==============================] - 9s 532us/step - loss: 0.5166 - acc: 0.7392\n",
      "Epoch 83/100\n",
      "16092/16092 [==============================] - 9s 543us/step - loss: 0.5157 - acc: 0.7387\n",
      "Epoch 84/100\n",
      "16092/16092 [==============================] - 7s 464us/step - loss: 0.5144 - acc: 0.7416\n",
      "Epoch 85/100\n",
      "16092/16092 [==============================] - 8s 515us/step - loss: 0.5155 - acc: 0.7383\n",
      "Epoch 86/100\n",
      "16092/16092 [==============================] - 8s 510us/step - loss: 0.5145 - acc: 0.7423\n",
      "Epoch 87/100\n",
      "16092/16092 [==============================] - 9s 565us/step - loss: 0.5142 - acc: 0.7409\n",
      "Epoch 88/100\n",
      "16092/16092 [==============================] - 9s 549us/step - loss: 0.5150 - acc: 0.7417\n",
      "Epoch 89/100\n",
      "16092/16092 [==============================] - 9s 538us/step - loss: 0.5134 - acc: 0.7422\n",
      "Epoch 90/100\n",
      "16092/16092 [==============================] - 8s 520us/step - loss: 0.5129 - acc: 0.7437\n",
      "Epoch 91/100\n",
      "16092/16092 [==============================] - 9s 543us/step - loss: 0.5126 - acc: 0.7409\n",
      "Epoch 92/100\n",
      "16092/16092 [==============================] - ETA: 0s - loss: 0.5125 - acc: 0.740 - 9s 545us/step - loss: 0.5125 - acc: 0.7401\n",
      "Epoch 93/100\n",
      "16092/16092 [==============================] - 8s 523us/step - loss: 0.5125 - acc: 0.7431\n",
      "Epoch 94/100\n",
      "16092/16092 [==============================] - 8s 512us/step - loss: 0.5112 - acc: 0.7436\n",
      "Epoch 95/100\n",
      "16092/16092 [==============================] - 8s 493us/step - loss: 0.5120 - acc: 0.7436\n",
      "Epoch 96/100\n",
      "16092/16092 [==============================] - 8s 488us/step - loss: 0.5121 - acc: 0.7440\n",
      "Epoch 97/100\n",
      "16092/16092 [==============================] - 8s 522us/step - loss: 0.5116 - acc: 0.7415\n",
      "Epoch 98/100\n",
      "16092/16092 [==============================] - 9s 540us/step - loss: 0.5110 - acc: 0.7431\n",
      "Epoch 99/100\n",
      "16092/16092 [==============================] - 8s 513us/step - loss: 0.5099 - acc: 0.7415\n",
      "Epoch 100/100\n",
      "16092/16092 [==============================] - 8s 511us/step - loss: 0.5096 - acc: 0.7427\n",
      "1788/1788 [==============================] - 1s 556us/step\n",
      "Epoch 1/100\n",
      "16092/16092 [==============================] - 11s 688us/step - loss: 0.6348 - acc: 0.6457\n",
      "Epoch 2/100\n",
      "16092/16092 [==============================] - 9s 532us/step - loss: 0.6186 - acc: 0.6631\n",
      "Epoch 3/100\n",
      "16092/16092 [==============================] - 9s 554us/step - loss: 0.6104 - acc: 0.6706\n",
      "Epoch 4/100\n",
      "16092/16092 [==============================] - 8s 528us/step - loss: 0.6026 - acc: 0.6783\n",
      "Epoch 5/100\n",
      "16092/16092 [==============================] - 9s 537us/step - loss: 0.5945 - acc: 0.6879\n",
      "Epoch 6/100\n",
      "16092/16092 [==============================] - 8s 518us/step - loss: 0.5880 - acc: 0.6905\n",
      "Epoch 7/100\n",
      "16092/16092 [==============================] - 8s 501us/step - loss: 0.5832 - acc: 0.6951\n",
      "Epoch 8/100\n",
      "16092/16092 [==============================] - 9s 535us/step - loss: 0.5793 - acc: 0.6989\n",
      "Epoch 9/100\n",
      "16092/16092 [==============================] - 8s 484us/step - loss: 0.5761 - acc: 0.7041\n",
      "Epoch 10/100\n",
      "16092/16092 [==============================] - 8s 497us/step - loss: 0.5732 - acc: 0.7034\n",
      "Epoch 11/100\n",
      "16092/16092 [==============================] - 8s 528us/step - loss: 0.5708 - acc: 0.7079\n",
      "Epoch 12/100\n",
      "16092/16092 [==============================] - 9s 534us/step - loss: 0.5692 - acc: 0.7076\n",
      "Epoch 13/100\n",
      "16092/16092 [==============================] - 8s 509us/step - loss: 0.5655 - acc: 0.7119\n",
      "Epoch 14/100\n",
      "16092/16092 [==============================] - 9s 541us/step - loss: 0.5638 - acc: 0.7121\n",
      "Epoch 15/100\n",
      "16092/16092 [==============================] - 8s 526us/step - loss: 0.5621 - acc: 0.7149\n",
      "Epoch 16/100\n",
      "16092/16092 [==============================] - 8s 516us/step - loss: 0.5585 - acc: 0.7149\n",
      "Epoch 17/100\n",
      "16092/16092 [==============================] - 8s 521us/step - loss: 0.5569 - acc: 0.7159\n",
      "Epoch 18/100\n",
      "16092/16092 [==============================] - 8s 473us/step - loss: 0.5559 - acc: 0.7157 0s - loss: 0.5\n",
      "Epoch 19/100\n",
      "16092/16092 [==============================] - 9s 557us/step - loss: 0.5545 - acc: 0.7181\n",
      "Epoch 20/100\n",
      "16092/16092 [==============================] - 9s 581us/step - loss: 0.5537 - acc: 0.7204\n",
      "Epoch 21/100\n",
      "16092/16092 [==============================] - 9s 556us/step - loss: 0.5515 - acc: 0.7223\n",
      "Epoch 22/100\n",
      "16092/16092 [==============================] - 10s 609us/step - loss: 0.5502 - acc: 0.7206\n",
      "Epoch 23/100\n",
      "16092/16092 [==============================] - 8s 525us/step - loss: 0.5495 - acc: 0.7207\n",
      "Epoch 24/100\n",
      "16092/16092 [==============================] - 8s 471us/step - loss: 0.5473 - acc: 0.7217 0s - loss: 0.5\n",
      "Epoch 25/100\n",
      "16092/16092 [==============================] - 8s 525us/step - loss: 0.5465 - acc: 0.7253\n",
      "Epoch 26/100\n",
      "16092/16092 [==============================] - 9s 537us/step - loss: 0.5451 - acc: 0.7258\n",
      "Epoch 27/100\n",
      "16092/16092 [==============================] - 9s 554us/step - loss: 0.5441 - acc: 0.7263\n",
      "Epoch 28/100\n",
      "16092/16092 [==============================] - 8s 513us/step - loss: 0.5435 - acc: 0.7248\n",
      "Epoch 29/100\n",
      "16092/16092 [==============================] - 8s 503us/step - loss: 0.5424 - acc: 0.7246\n",
      "Epoch 30/100\n",
      "16092/16092 [==============================] - 9s 539us/step - loss: 0.5419 - acc: 0.7268\n",
      "Epoch 31/100\n",
      "16092/16092 [==============================] - 8s 505us/step - loss: 0.5412 - acc: 0.7228\n",
      "Epoch 32/100\n",
      "16092/16092 [==============================] - 8s 516us/step - loss: 0.5410 - acc: 0.7271\n",
      "Epoch 33/100\n",
      "16092/16092 [==============================] - 9s 534us/step - loss: 0.5393 - acc: 0.7275\n",
      "Epoch 34/100\n",
      "16092/16092 [==============================] - 8s 522us/step - loss: 0.5392 - acc: 0.7282\n",
      "Epoch 35/100\n",
      "16092/16092 [==============================] - 8s 514us/step - loss: 0.5383 - acc: 0.7289\n",
      "Epoch 36/100\n",
      "16092/16092 [==============================] - 8s 510us/step - loss: 0.5377 - acc: 0.7260\n",
      "Epoch 37/100\n",
      "16092/16092 [==============================] - 9s 539us/step - loss: 0.5372 - acc: 0.7326\n",
      "Epoch 38/100\n",
      "16092/16092 [==============================] - 9s 532us/step - loss: 0.5350 - acc: 0.7324\n",
      "Epoch 39/100\n",
      "16092/16092 [==============================] - 9s 533us/step - loss: 0.5347 - acc: 0.7328\n",
      "Epoch 40/100\n",
      "16092/16092 [==============================] - 6s 401us/step - loss: 0.5344 - acc: 0.7319\n",
      "Epoch 41/100\n",
      "16092/16092 [==============================] - 5s 330us/step - loss: 0.5343 - acc: 0.7315\n",
      "Epoch 42/100\n",
      "16092/16092 [==============================] - 6s 343us/step - loss: 0.5332 - acc: 0.7338\n",
      "Epoch 43/100\n",
      "16092/16092 [==============================] - 5s 337us/step - loss: 0.5327 - acc: 0.7327\n",
      "Epoch 44/100\n",
      "16092/16092 [==============================] - 5s 325us/step - loss: 0.5323 - acc: 0.7357\n",
      "Epoch 45/100\n",
      "16092/16092 [==============================] - 6s 343us/step - loss: 0.5324 - acc: 0.7339\n",
      "Epoch 46/100\n",
      "16092/16092 [==============================] - 5s 307us/step - loss: 0.5305 - acc: 0.7363\n",
      "Epoch 47/100\n",
      "16092/16092 [==============================] - 5s 321us/step - loss: 0.5318 - acc: 0.7352\n",
      "Epoch 48/100\n",
      "16092/16092 [==============================] - 6s 342us/step - loss: 0.5302 - acc: 0.7331\n",
      "Epoch 49/100\n",
      "16092/16092 [==============================] - 6s 372us/step - loss: 0.5307 - acc: 0.7323\n",
      "Epoch 50/100\n",
      "16092/16092 [==============================] - 6s 387us/step - loss: 0.5288 - acc: 0.7354\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 6s 366us/step - loss: 0.5285 - acc: 0.7339\n",
      "Epoch 52/100\n",
      "16092/16092 [==============================] - 7s 456us/step - loss: 0.5285 - acc: 0.7345\n",
      "Epoch 53/100\n",
      "16092/16092 [==============================] - 8s 493us/step - loss: 0.5272 - acc: 0.7384\n",
      "Epoch 54/100\n",
      "16092/16092 [==============================] - 5s 304us/step - loss: 0.5279 - acc: 0.7384\n",
      "Epoch 55/100\n",
      "16092/16092 [==============================] - 6s 361us/step - loss: 0.5264 - acc: 0.7368\n",
      "Epoch 56/100\n",
      "16092/16092 [==============================] - 6s 363us/step - loss: 0.5257 - acc: 0.7376\n",
      "Epoch 57/100\n",
      "16092/16092 [==============================] - 5s 286us/step - loss: 0.5253 - acc: 0.7382\n",
      "Epoch 58/100\n",
      "16092/16092 [==============================] - 4s 257us/step - loss: 0.5244 - acc: 0.7398\n",
      "Epoch 59/100\n",
      "16092/16092 [==============================] - 4s 247us/step - loss: 0.5239 - acc: 0.7380\n",
      "Epoch 60/100\n",
      "16092/16092 [==============================] - 4s 248us/step - loss: 0.5224 - acc: 0.7418\n",
      "Epoch 61/100\n",
      "16092/16092 [==============================] - 4s 249us/step - loss: 0.5227 - acc: 0.7410\n",
      "Epoch 62/100\n",
      "16092/16092 [==============================] - 4s 248us/step - loss: 0.5215 - acc: 0.7412\n",
      "Epoch 63/100\n",
      "16092/16092 [==============================] - 4s 248us/step - loss: 0.5221 - acc: 0.7407\n",
      "Epoch 64/100\n",
      "16092/16092 [==============================] - 4s 255us/step - loss: 0.5203 - acc: 0.7434\n",
      "Epoch 65/100\n",
      "16092/16092 [==============================] - 5s 307us/step - loss: 0.5195 - acc: 0.7405\n",
      "Epoch 66/100\n",
      "16092/16092 [==============================] - 5s 306us/step - loss: 0.5207 - acc: 0.7436\n",
      "Epoch 67/100\n",
      "16092/16092 [==============================] - 6s 342us/step - loss: 0.5192 - acc: 0.7397 1s - l\n",
      "Epoch 68/100\n",
      "16092/16092 [==============================] - 8s 488us/step - loss: 0.5182 - acc: 0.7427 2s - loss: 0.5135 - a -\n",
      "Epoch 69/100\n",
      "16092/16092 [==============================] - 9s 552us/step - loss: 0.5169 - acc: 0.7455\n",
      "Epoch 70/100\n",
      "16092/16092 [==============================] - 7s 438us/step - loss: 0.5166 - acc: 0.7402\n",
      "Epoch 71/100\n",
      "16092/16092 [==============================] - 8s 492us/step - loss: 0.5161 - acc: 0.7468 0s - loss: 0.5135 - acc: 0. - ETA: 0s - loss: 0.5138\n",
      "Epoch 72/100\n",
      "16092/16092 [==============================] - 8s 517us/step - loss: 0.5153 - acc: 0.7455\n",
      "Epoch 73/100\n",
      "16092/16092 [==============================] - 9s 532us/step - loss: 0.5157 - acc: 0.7437\n",
      "Epoch 74/100\n",
      "16092/16092 [==============================] - 8s 490us/step - loss: 0.5138 - acc: 0.7486\n",
      "Epoch 75/100\n",
      "16092/16092 [==============================] - 6s 387us/step - loss: 0.5140 - acc: 0.7457\n",
      "Epoch 76/100\n",
      "16092/16092 [==============================] - 5s 338us/step - loss: 0.5132 - acc: 0.7463\n",
      "Epoch 77/100\n",
      "16092/16092 [==============================] - 5s 290us/step - loss: 0.5120 - acc: 0.7460\n",
      "Epoch 78/100\n",
      "16092/16092 [==============================] - 5s 294us/step - loss: 0.5119 - acc: 0.7469\n",
      "Epoch 79/100\n",
      "16092/16092 [==============================] - 5s 282us/step - loss: 0.5108 - acc: 0.7483\n",
      "Epoch 80/100\n",
      "16092/16092 [==============================] - 4s 268us/step - loss: 0.5102 - acc: 0.7491\n",
      "Epoch 81/100\n",
      "16092/16092 [==============================] - 4s 265us/step - loss: 0.5098 - acc: 0.7493\n",
      "Epoch 82/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5080 - acc: 0.7501\n",
      "Epoch 83/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5090 - acc: 0.7486\n",
      "Epoch 84/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5065 - acc: 0.7499\n",
      "Epoch 85/100\n",
      "16092/16092 [==============================] - 6s 362us/step - loss: 0.5076 - acc: 0.7491\n",
      "Epoch 86/100\n",
      "16092/16092 [==============================] - 5s 323us/step - loss: 0.5057 - acc: 0.7538\n",
      "Epoch 87/100\n",
      "16092/16092 [==============================] - 5s 281us/step - loss: 0.5051 - acc: 0.7504\n",
      "Epoch 88/100\n",
      "16092/16092 [==============================] - 5s 319us/step - loss: 0.5063 - acc: 0.7522\n",
      "Epoch 89/100\n",
      "16092/16092 [==============================] - 5s 304us/step - loss: 0.5051 - acc: 0.7499\n",
      "Epoch 90/100\n",
      "16092/16092 [==============================] - 5s 285us/step - loss: 0.5034 - acc: 0.7529\n",
      "Epoch 91/100\n",
      "16092/16092 [==============================] - 5s 304us/step - loss: 0.5057 - acc: 0.7506\n",
      "Epoch 92/100\n",
      "16092/16092 [==============================] - 5s 290us/step - loss: 0.5029 - acc: 0.7535\n",
      "Epoch 93/100\n",
      "16092/16092 [==============================] - 5s 340us/step - loss: 0.5039 - acc: 0.7530\n",
      "Epoch 94/100\n",
      "16092/16092 [==============================] - 6s 359us/step - loss: 0.5033 - acc: 0.7509\n",
      "Epoch 95/100\n",
      "16092/16092 [==============================] - 5s 286us/step - loss: 0.5035 - acc: 0.7530\n",
      "Epoch 96/100\n",
      "16092/16092 [==============================] - 5s 323us/step - loss: 0.5030 - acc: 0.7544\n",
      "Epoch 97/100\n",
      "16092/16092 [==============================] - 6s 348us/step - loss: 0.5017 - acc: 0.7524\n",
      "Epoch 98/100\n",
      "16092/16092 [==============================] - 5s 298us/step - loss: 0.5019 - acc: 0.7507\n",
      "Epoch 99/100\n",
      "16092/16092 [==============================] - 5s 286us/step - loss: 0.5008 - acc: 0.7547\n",
      "Epoch 100/100\n",
      "16092/16092 [==============================] - 5s 336us/step - loss: 0.5004 - acc: 0.7547\n",
      "1788/1788 [==============================] - 1s 287us/step\n",
      "Epoch 1/100\n",
      "16092/16092 [==============================] - 7s 428us/step - loss: 0.6334 - acc: 0.6500\n",
      "Epoch 2/100\n",
      "16092/16092 [==============================] - 6s 402us/step - loss: 0.6158 - acc: 0.6655\n",
      "Epoch 3/100\n",
      "16092/16092 [==============================] - 5s 324us/step - loss: 0.6084 - acc: 0.6711\n",
      "Epoch 4/100\n",
      "16092/16092 [==============================] - 4s 268us/step - loss: 0.6021 - acc: 0.6752\n",
      "Epoch 5/100\n",
      "16092/16092 [==============================] - 5s 305us/step - loss: 0.5970 - acc: 0.6772\n",
      "Epoch 6/100\n",
      "16092/16092 [==============================] - 4s 273us/step - loss: 0.5907 - acc: 0.6802\n",
      "Epoch 7/100\n",
      "16092/16092 [==============================] - 5s 313us/step - loss: 0.5867 - acc: 0.6870\n",
      "Epoch 8/100\n",
      "16092/16092 [==============================] - 4s 273us/step - loss: 0.5827 - acc: 0.6902\n",
      "Epoch 9/100\n",
      "16092/16092 [==============================] - 5s 302us/step - loss: 0.5786 - acc: 0.6927\n",
      "Epoch 10/100\n",
      "16092/16092 [==============================] - ETA: 0s - loss: 0.5744 - acc: 0.6964- ETA: 0s - loss: 0.5738 - acc:  - 5s 315us/step - loss: 0.5744 - acc: 0.6965\n",
      "Epoch 11/100\n",
      "16092/16092 [==============================] - 5s 289us/step - loss: 0.5715 - acc: 0.6994\n",
      "Epoch 12/100\n",
      "16092/16092 [==============================] - 5s 307us/step - loss: 0.5690 - acc: 0.7016\n",
      "Epoch 13/100\n",
      "16092/16092 [==============================] - 5s 297us/step - loss: 0.5660 - acc: 0.7039\n",
      "Epoch 14/100\n",
      "16092/16092 [==============================] - 5s 288us/step - loss: 0.5653 - acc: 0.7010\n",
      "Epoch 15/100\n",
      "16092/16092 [==============================] - 5s 310us/step - loss: 0.5636 - acc: 0.7083\n",
      "Epoch 16/100\n",
      "16092/16092 [==============================] - 5s 291us/step - loss: 0.5605 - acc: 0.7068\n",
      "Epoch 17/100\n",
      "16092/16092 [==============================] - 5s 305us/step - loss: 0.5587 - acc: 0.7122\n",
      "Epoch 18/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5567 - acc: 0.7124\n",
      "Epoch 19/100\n",
      "16092/16092 [==============================] - 6s 344us/step - loss: 0.5565 - acc: 0.7109\n",
      "Epoch 20/100\n",
      "16092/16092 [==============================] - 6s 392us/step - loss: 0.5552 - acc: 0.7125\n",
      "Epoch 21/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5542 - acc: 0.7128 0s - loss: 0.55\n",
      "Epoch 22/100\n",
      "16092/16092 [==============================] - 6s 362us/step - loss: 0.5528 - acc: 0.7169\n",
      "Epoch 23/100\n",
      "16092/16092 [==============================] - 6s 380us/step - loss: 0.5507 - acc: 0.7164\n",
      "Epoch 24/100\n",
      "16092/16092 [==============================] - 6s 362us/step - loss: 0.5493 - acc: 0.7188\n",
      "Epoch 25/100\n",
      "16092/16092 [==============================] - 6s 397us/step - loss: 0.5491 - acc: 0.7192\n",
      "Epoch 26/100\n",
      "16092/16092 [==============================] - 6s 357us/step - loss: 0.5479 - acc: 0.7211\n",
      "Epoch 27/100\n",
      "16092/16092 [==============================] - 5s 283us/step - loss: 0.5467 - acc: 0.7179\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 5s 306us/step - loss: 0.5454 - acc: 0.7222\n",
      "Epoch 29/100\n",
      "16092/16092 [==============================] - 6s 382us/step - loss: 0.5453 - acc: 0.7232\n",
      "Epoch 30/100\n",
      "16092/16092 [==============================] - 5s 334us/step - loss: 0.5436 - acc: 0.7252\n",
      "Epoch 31/100\n",
      "16092/16092 [==============================] - 5s 334us/step - loss: 0.5435 - acc: 0.7253 0s - loss: 0.54\n",
      "Epoch 32/100\n",
      "16092/16092 [==============================] - 6s 353us/step - loss: 0.5419 - acc: 0.7214\n",
      "Epoch 33/100\n",
      "16092/16092 [==============================] - 6s 354us/step - loss: 0.5414 - acc: 0.7235\n",
      "Epoch 34/100\n",
      "16092/16092 [==============================] - 6s 353us/step - loss: 0.5407 - acc: 0.7269\n",
      "Epoch 35/100\n",
      "16092/16092 [==============================] - 6s 352us/step - loss: 0.5388 - acc: 0.7278\n",
      "Epoch 36/100\n",
      "16092/16092 [==============================] - 6s 353us/step - loss: 0.5389 - acc: 0.7269\n",
      "Epoch 37/100\n",
      "16092/16092 [==============================] - 6s 353us/step - loss: 0.5393 - acc: 0.7258\n",
      "Epoch 38/100\n",
      "16092/16092 [==============================] - 6s 350us/step - loss: 0.5373 - acc: 0.7277\n",
      "Epoch 39/100\n",
      "16092/16092 [==============================] - 6s 350us/step - loss: 0.5367 - acc: 0.7264\n",
      "Epoch 40/100\n",
      "16092/16092 [==============================] - 6s 350us/step - loss: 0.5359 - acc: 0.7289\n",
      "Epoch 41/100\n",
      "16092/16092 [==============================] - 6s 349us/step - loss: 0.5347 - acc: 0.7280\n",
      "Epoch 42/100\n",
      "16092/16092 [==============================] - 6s 351us/step - loss: 0.5344 - acc: 0.7270\n",
      "Epoch 43/100\n",
      "16092/16092 [==============================] - 6s 351us/step - loss: 0.5324 - acc: 0.7327\n",
      "Epoch 44/100\n",
      "16092/16092 [==============================] - 6s 348us/step - loss: 0.5320 - acc: 0.7324\n",
      "Epoch 45/100\n",
      "16092/16092 [==============================] - 6s 382us/step - loss: 0.5314 - acc: 0.7317\n",
      "Epoch 46/100\n",
      "16092/16092 [==============================] - 7s 447us/step - loss: 0.5309 - acc: 0.7317\n",
      "Epoch 47/100\n",
      "16092/16092 [==============================] - 6s 400us/step - loss: 0.5302 - acc: 0.7330 1\n",
      "Epoch 48/100\n",
      "16092/16092 [==============================] - 7s 414us/step - loss: 0.5303 - acc: 0.7338\n",
      "Epoch 49/100\n",
      "16092/16092 [==============================] - 6s 372us/step - loss: 0.5288 - acc: 0.7333\n",
      "Epoch 50/100\n",
      "16092/16092 [==============================] - 7s 413us/step - loss: 0.5285 - acc: 0.7341\n",
      "Epoch 51/100\n",
      "16092/16092 [==============================] - 6s 380us/step - loss: 0.5279 - acc: 0.7352\n",
      "Epoch 52/100\n",
      "16092/16092 [==============================] - 6s 368us/step - loss: 0.5270 - acc: 0.7388\n",
      "Epoch 53/100\n",
      "16092/16092 [==============================] - 7s 439us/step - loss: 0.5269 - acc: 0.7344\n",
      "Epoch 54/100\n",
      "16092/16092 [==============================] - 6s 403us/step - loss: 0.5262 - acc: 0.7352\n",
      "Epoch 55/100\n",
      "16092/16092 [==============================] - 5s 340us/step - loss: 0.5252 - acc: 0.7354\n",
      "Epoch 56/100\n",
      "16092/16092 [==============================] - 5s 337us/step - loss: 0.5238 - acc: 0.7396\n",
      "Epoch 57/100\n",
      "16092/16092 [==============================] - 7s 458us/step - loss: 0.5247 - acc: 0.7374\n",
      "Epoch 58/100\n",
      "16092/16092 [==============================] - 6s 355us/step - loss: 0.5237 - acc: 0.7389\n",
      "Epoch 59/100\n",
      "16092/16092 [==============================] - 6s 380us/step - loss: 0.5239 - acc: 0.7356\n",
      "Epoch 60/100\n",
      "16092/16092 [==============================] - 6s 386us/step - loss: 0.5229 - acc: 0.7366\n",
      "Epoch 61/100\n",
      "16092/16092 [==============================] - 6s 381us/step - loss: 0.5220 - acc: 0.7397 1s - loss: 0\n",
      "Epoch 62/100\n",
      "16092/16092 [==============================] - 6s 365us/step - loss: 0.5215 - acc: 0.7379\n",
      "Epoch 63/100\n",
      "16092/16092 [==============================] - 6s 390us/step - loss: 0.5204 - acc: 0.7378\n",
      "Epoch 64/100\n",
      "16092/16092 [==============================] - 5s 339us/step - loss: 0.5201 - acc: 0.7391 0s - loss: 0.5212 - acc: 0.73\n",
      "Epoch 65/100\n",
      "16092/16092 [==============================] - 6s 364us/step - loss: 0.5202 - acc: 0.7398\n",
      "Epoch 66/100\n",
      "16092/16092 [==============================] - 6s 351us/step - loss: 0.5191 - acc: 0.7415\n",
      "Epoch 67/100\n",
      "16092/16092 [==============================] - 6s 351us/step - loss: 0.5190 - acc: 0.7392\n",
      "Epoch 68/100\n",
      "16092/16092 [==============================] - 6s 348us/step - loss: 0.5201 - acc: 0.7411\n",
      "Epoch 69/100\n",
      "16092/16092 [==============================] - 6s 378us/step - loss: 0.5186 - acc: 0.7392\n",
      "Epoch 70/100\n",
      "16092/16092 [==============================] - 6s 346us/step - loss: 0.5180 - acc: 0.7387\n",
      "Epoch 71/100\n",
      "16092/16092 [==============================] - 6s 365us/step - loss: 0.5173 - acc: 0.7399\n",
      "Epoch 72/100\n",
      "16092/16092 [==============================] - 6s 371us/step - loss: 0.5161 - acc: 0.7415\n",
      "Epoch 73/100\n",
      "16092/16092 [==============================] - 5s 333us/step - loss: 0.5162 - acc: 0.7406\n",
      "Epoch 74/100\n",
      "16092/16092 [==============================] - 6s 354us/step - loss: 0.5156 - acc: 0.7391\n",
      "Epoch 75/100\n",
      "16092/16092 [==============================] - 5s 333us/step - loss: 0.5144 - acc: 0.7440\n",
      "Epoch 76/100\n",
      "16092/16092 [==============================] - 5s 339us/step - loss: 0.5157 - acc: 0.7424\n",
      "Epoch 77/100\n",
      "16092/16092 [==============================] - 5s 337us/step - loss: 0.5152 - acc: 0.7422\n",
      "Epoch 78/100\n",
      "16092/16092 [==============================] - 5s 331us/step - loss: 0.5152 - acc: 0.7429\n",
      "Epoch 79/100\n",
      "16092/16092 [==============================] - 5s 332us/step - loss: 0.5136 - acc: 0.7449\n",
      "Epoch 80/100\n",
      "16092/16092 [==============================] - 5s 333us/step - loss: 0.5140 - acc: 0.7440\n",
      "Epoch 81/100\n",
      "16092/16092 [==============================] - 5s 330us/step - loss: 0.5133 - acc: 0.7410\n",
      "Epoch 82/100\n",
      "16092/16092 [==============================] - 5s 333us/step - loss: 0.5119 - acc: 0.7432\n",
      "Epoch 83/100\n",
      "16092/16092 [==============================] - 5s 330us/step - loss: 0.5117 - acc: 0.7442\n",
      "Epoch 84/100\n",
      "16092/16092 [==============================] - 6s 354us/step - loss: 0.5127 - acc: 0.7433\n",
      "Epoch 85/100\n",
      "16092/16092 [==============================] - 6s 356us/step - loss: 0.5123 - acc: 0.7432\n",
      "Epoch 86/100\n",
      "16092/16092 [==============================] - 5s 334us/step - loss: 0.5118 - acc: 0.7434\n",
      "Epoch 87/100\n",
      "16092/16092 [==============================] - 5s 339us/step - loss: 0.5106 - acc: 0.7448\n",
      "Epoch 88/100\n",
      "16092/16092 [==============================] - 6s 355us/step - loss: 0.5107 - acc: 0.7424\n",
      "Epoch 89/100\n",
      "16092/16092 [==============================] - 6s 350us/step - loss: 0.5103 - acc: 0.7431\n",
      "Epoch 90/100\n",
      "16092/16092 [==============================] - 5s 337us/step - loss: 0.5100 - acc: 0.7438\n",
      "Epoch 91/100\n",
      "16092/16092 [==============================] - 6s 345us/step - loss: 0.5106 - acc: 0.7471\n",
      "Epoch 92/100\n",
      "16092/16092 [==============================] - 6s 348us/step - loss: 0.5094 - acc: 0.7473\n",
      "Epoch 93/100\n",
      "16092/16092 [==============================] - 6s 366us/step - loss: 0.5092 - acc: 0.7448\n",
      "Epoch 94/100\n",
      "16092/16092 [==============================] - 6s 342us/step - loss: 0.5087 - acc: 0.7460\n",
      "Epoch 95/100\n",
      "16092/16092 [==============================] - 5s 338us/step - loss: 0.5091 - acc: 0.7460\n",
      "Epoch 96/100\n",
      "16092/16092 [==============================] - 6s 346us/step - loss: 0.5085 - acc: 0.7477\n",
      "Epoch 97/100\n",
      "16092/16092 [==============================] - 5s 333us/step - loss: 0.5080 - acc: 0.7464\n",
      "Epoch 98/100\n",
      "16092/16092 [==============================] - 5s 332us/step - loss: 0.5083 - acc: 0.7463\n",
      "Epoch 99/100\n",
      "16092/16092 [==============================] - 5s 332us/step - loss: 0.5068 - acc: 0.7477\n",
      "Epoch 100/100\n",
      "16092/16092 [==============================] - 5s 330us/step - loss: 0.5067 - acc: 0.7469\n",
      "1788/1788 [==============================] - 1s 321us/step\n",
      "Epoch 1/100\n",
      "16092/16092 [==============================] - 6s 402us/step - loss: 0.6377 - acc: 0.6427\n",
      "Epoch 2/100\n",
      "16092/16092 [==============================] - 5s 328us/step - loss: 0.6203 - acc: 0.6621\n",
      "Epoch 3/100\n",
      "16092/16092 [==============================] - 5s 327us/step - loss: 0.6113 - acc: 0.6697\n",
      "Epoch 4/100\n",
      "16092/16092 [==============================] - 5s 339us/step - loss: 0.6037 - acc: 0.6765\n",
      "Epoch 5/100\n",
      "16092/16092 [==============================] - 5s 334us/step - loss: 0.5979 - acc: 0.6789\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 5s 328us/step - loss: 0.5926 - acc: 0.6826\n",
      "Epoch 7/100\n",
      "16092/16092 [==============================] - 5s 337us/step - loss: 0.5869 - acc: 0.6899\n",
      "Epoch 8/100\n",
      "16092/16092 [==============================] - 5s 323us/step - loss: 0.5816 - acc: 0.6949\n",
      "Epoch 9/100\n",
      "16092/16092 [==============================] - 5s 337us/step - loss: 0.5769 - acc: 0.6990\n",
      "Epoch 10/100\n",
      "16092/16092 [==============================] - 6s 342us/step - loss: 0.5736 - acc: 0.7006\n",
      "Epoch 11/100\n",
      "16092/16092 [==============================] - 5s 335us/step - loss: 0.5689 - acc: 0.7071\n",
      "Epoch 12/100\n",
      "16092/16092 [==============================] - 6s 363us/step - loss: 0.5668 - acc: 0.7061\n",
      "Epoch 13/100\n",
      "16092/16092 [==============================] - 5s 314us/step - loss: 0.5641 - acc: 0.7086\n",
      "Epoch 14/100\n",
      "16092/16092 [==============================] - 5s 315us/step - loss: 0.5624 - acc: 0.7113\n",
      "Epoch 15/100\n",
      "16092/16092 [==============================] - 5s 307us/step - loss: 0.5598 - acc: 0.7113\n",
      "Epoch 16/100\n",
      "16092/16092 [==============================] - 5s 326us/step - loss: 0.5591 - acc: 0.7117\n",
      "Epoch 17/100\n",
      "16092/16092 [==============================] - 5s 321us/step - loss: 0.5563 - acc: 0.7137\n",
      "Epoch 18/100\n",
      "16092/16092 [==============================] - 6s 348us/step - loss: 0.5557 - acc: 0.7133\n",
      "Epoch 19/100\n",
      "16092/16092 [==============================] - 6s 348us/step - loss: 0.5534 - acc: 0.7179\n",
      "Epoch 20/100\n",
      "16092/16092 [==============================] - 5s 327us/step - loss: 0.5511 - acc: 0.7175\n",
      "Epoch 21/100\n",
      "16092/16092 [==============================] - 5s 338us/step - loss: 0.5499 - acc: 0.7177\n",
      "Epoch 22/100\n",
      "16092/16092 [==============================] - 7s 427us/step - loss: 0.5477 - acc: 0.7219\n",
      "Epoch 23/100\n",
      "16092/16092 [==============================] - 6s 350us/step - loss: 0.5468 - acc: 0.7234\n",
      "Epoch 24/100\n",
      "16092/16092 [==============================] - 6s 376us/step - loss: 0.5451 - acc: 0.7213\n",
      "Epoch 25/100\n",
      "16092/16092 [==============================] - 7s 410us/step - loss: 0.5457 - acc: 0.7254\n",
      "Epoch 26/100\n",
      "16092/16092 [==============================] - 8s 504us/step - loss: 0.5436 - acc: 0.7268\n",
      "Epoch 27/100\n",
      "16092/16092 [==============================] - 6s 352us/step - loss: 0.5445 - acc: 0.7238\n",
      "Epoch 28/100\n",
      "16092/16092 [==============================] - 5s 291us/step - loss: 0.5419 - acc: 0.7261\n",
      "Epoch 29/100\n",
      "16092/16092 [==============================] - 4s 279us/step - loss: 0.5407 - acc: 0.7261\n",
      "Epoch 30/100\n",
      "16092/16092 [==============================] - 5s 281us/step - loss: 0.5394 - acc: 0.7293\n",
      "Epoch 31/100\n",
      "16092/16092 [==============================] - 5s 300us/step - loss: 0.5390 - acc: 0.7311\n",
      "Epoch 32/100\n",
      "16092/16092 [==============================] - 5s 318us/step - loss: 0.5386 - acc: 0.7314\n",
      "Epoch 33/100\n",
      "16092/16092 [==============================] - 4s 278us/step - loss: 0.5361 - acc: 0.7309\n",
      "Epoch 34/100\n",
      "16092/16092 [==============================] - 4s 279us/step - loss: 0.5361 - acc: 0.7304\n",
      "Epoch 35/100\n",
      "16092/16092 [==============================] - 5s 284us/step - loss: 0.5354 - acc: 0.7311\n",
      "Epoch 36/100\n",
      "16092/16092 [==============================] - 6s 393us/step - loss: 0.5344 - acc: 0.7308\n",
      "Epoch 37/100\n",
      "16092/16092 [==============================] - 8s 501us/step - loss: 0.5334 - acc: 0.7345\n",
      "Epoch 38/100\n",
      "16092/16092 [==============================] - 8s 489us/step - loss: 0.5336 - acc: 0.7327\n",
      "Epoch 39/100\n",
      "16092/16092 [==============================] - 9s 554us/step - loss: 0.5320 - acc: 0.7320\n",
      "Epoch 40/100\n",
      "16092/16092 [==============================] - 9s 532us/step - loss: 0.5315 - acc: 0.7326 0s - loss: 0.5304 \n",
      "Epoch 41/100\n",
      "16092/16092 [==============================] - 10s 596us/step - loss: 0.5307 - acc: 0.7343\n",
      "Epoch 42/100\n",
      "16092/16092 [==============================] - 9s 543us/step - loss: 0.5290 - acc: 0.7360\n",
      "Epoch 43/100\n",
      "16092/16092 [==============================] - 9s 572us/step - loss: 0.5288 - acc: 0.7360\n",
      "Epoch 44/100\n",
      "16092/16092 [==============================] - 9s 530us/step - loss: 0.5271 - acc: 0.7338\n",
      "Epoch 45/100\n",
      "16092/16092 [==============================] - 9s 539us/step - loss: 0.5276 - acc: 0.7348\n",
      "Epoch 46/100\n",
      "16092/16092 [==============================] - 9s 543us/step - loss: 0.5261 - acc: 0.7346\n",
      "Epoch 47/100\n",
      "16092/16092 [==============================] - 9s 539us/step - loss: 0.5254 - acc: 0.7344\n",
      "Epoch 48/100\n",
      "16092/16092 [==============================] - 7s 456us/step - loss: 0.5240 - acc: 0.7374\n",
      "Epoch 49/100\n",
      "16092/16092 [==============================] - 6s 385us/step - loss: 0.5238 - acc: 0.7373\n",
      "Epoch 50/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5237 - acc: 0.7380\n",
      "Epoch 51/100\n",
      "16092/16092 [==============================] - 4s 265us/step - loss: 0.5225 - acc: 0.7365\n",
      "Epoch 52/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5231 - acc: 0.7353\n",
      "Epoch 53/100\n",
      "16092/16092 [==============================] - 4s 265us/step - loss: 0.5220 - acc: 0.7373\n",
      "Epoch 54/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5210 - acc: 0.7406\n",
      "Epoch 55/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5208 - acc: 0.7374\n",
      "Epoch 56/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5206 - acc: 0.7389\n",
      "Epoch 57/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5208 - acc: 0.7396\n",
      "Epoch 58/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5190 - acc: 0.7417\n",
      "Epoch 59/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5186 - acc: 0.7412\n",
      "Epoch 60/100\n",
      "16092/16092 [==============================] - 4s 268us/step - loss: 0.5183 - acc: 0.7390\n",
      "Epoch 61/100\n",
      "16092/16092 [==============================] - 4s 265us/step - loss: 0.5176 - acc: 0.7381\n",
      "Epoch 62/100\n",
      "16092/16092 [==============================] - 4s 268us/step - loss: 0.5171 - acc: 0.7405\n",
      "Epoch 63/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5175 - acc: 0.7428\n",
      "Epoch 64/100\n",
      "16092/16092 [==============================] - 4s 268us/step - loss: 0.5160 - acc: 0.7426\n",
      "Epoch 65/100\n",
      "16092/16092 [==============================] - ETA: 0s - loss: 0.5155 - acc: 0.739 - 4s 266us/step - loss: 0.5155 - acc: 0.7394\n",
      "Epoch 66/100\n",
      "16092/16092 [==============================] - 5s 333us/step - loss: 0.5151 - acc: 0.7397\n",
      "Epoch 67/100\n",
      "16092/16092 [==============================] - 5s 298us/step - loss: 0.5149 - acc: 0.7424\n",
      "Epoch 68/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5143 - acc: 0.7394\n",
      "Epoch 69/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5145 - acc: 0.7398\n",
      "Epoch 70/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5131 - acc: 0.7406\n",
      "Epoch 71/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5130 - acc: 0.7446\n",
      "Epoch 72/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5128 - acc: 0.7395\n",
      "Epoch 73/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5120 - acc: 0.7424\n",
      "Epoch 74/100\n",
      "16092/16092 [==============================] - 4s 268us/step - loss: 0.5117 - acc: 0.7416\n",
      "Epoch 75/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5116 - acc: 0.7433\n",
      "Epoch 76/100\n",
      "16092/16092 [==============================] - 4s 269us/step - loss: 0.5113 - acc: 0.7414\n",
      "Epoch 77/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5106 - acc: 0.7420\n",
      "Epoch 78/100\n",
      "16092/16092 [==============================] - 4s 268us/step - loss: 0.5109 - acc: 0.7430\n",
      "Epoch 79/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5104 - acc: 0.7442\n",
      "Epoch 80/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5104 - acc: 0.7406\n",
      "Epoch 81/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5095 - acc: 0.7450\n",
      "Epoch 82/100\n",
      "16092/16092 [==============================] - 4s 265us/step - loss: 0.5089 - acc: 0.7452\n",
      "Epoch 83/100\n",
      "16092/16092 [==============================] - 5s 286us/step - loss: 0.5090 - acc: 0.7446\n",
      "Epoch 84/100\n",
      "16092/16092 [==============================] - 4s 268us/step - loss: 0.5084 - acc: 0.7432\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 4s 268us/step - loss: 0.5088 - acc: 0.7455\n",
      "Epoch 86/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5082 - acc: 0.7447\n",
      "Epoch 87/100\n",
      "16092/16092 [==============================] - 4s 273us/step - loss: 0.5068 - acc: 0.7455\n",
      "Epoch 88/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5076 - acc: 0.7465\n",
      "Epoch 89/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5081 - acc: 0.7454\n",
      "Epoch 90/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5061 - acc: 0.7464\n",
      "Epoch 91/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5060 - acc: 0.7472\n",
      "Epoch 92/100\n",
      "16092/16092 [==============================] - 4s 268us/step - loss: 0.5062 - acc: 0.7452\n",
      "Epoch 93/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5047 - acc: 0.7498\n",
      "Epoch 94/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5059 - acc: 0.7469\n",
      "Epoch 95/100\n",
      "16092/16092 [==============================] - 4s 266us/step - loss: 0.5067 - acc: 0.7469\n",
      "Epoch 96/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5042 - acc: 0.7478\n",
      "Epoch 97/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5050 - acc: 0.7463\n",
      "Epoch 98/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5034 - acc: 0.7478\n",
      "Epoch 99/100\n",
      "16092/16092 [==============================] - 4s 267us/step - loss: 0.5043 - acc: 0.7453\n",
      "Epoch 100/100\n",
      "16092/16092 [==============================] - 4s 265us/step - loss: 0.5024 - acc: 0.7473\n",
      "1788/1788 [==============================] - 1s 303us/step\n",
      "Epoch 1/100\n",
      "16092/16092 [==============================] - 6s 384us/step - loss: 0.6359 - acc: 0.6444\n",
      "Epoch 2/100\n",
      "16092/16092 [==============================] - 5s 291us/step - loss: 0.6167 - acc: 0.6649\n",
      "Epoch 3/100\n",
      "16092/16092 [==============================] - 4s 273us/step - loss: 0.6107 - acc: 0.6667\n",
      "Epoch 4/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.6050 - acc: 0.6697\n",
      "Epoch 5/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.6004 - acc: 0.6754\n",
      "Epoch 6/100\n",
      "16092/16092 [==============================] - 4s 274us/step - loss: 0.5965 - acc: 0.6805\n",
      "Epoch 7/100\n",
      "16092/16092 [==============================] - 5s 286us/step - loss: 0.5926 - acc: 0.6797\n",
      "Epoch 8/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5893 - acc: 0.6855\n",
      "Epoch 9/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5866 - acc: 0.6893\n",
      "Epoch 10/100\n",
      "16092/16092 [==============================] - 5s 286us/step - loss: 0.5833 - acc: 0.6891\n",
      "Epoch 11/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5810 - acc: 0.6916\n",
      "Epoch 12/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5770 - acc: 0.6962\n",
      "Epoch 13/100\n",
      "16092/16092 [==============================] - 5s 287us/step - loss: 0.5746 - acc: 0.6983\n",
      "Epoch 14/100\n",
      "16092/16092 [==============================] - 4s 273us/step - loss: 0.5709 - acc: 0.7025\n",
      "Epoch 15/100\n",
      "16092/16092 [==============================] - 4s 274us/step - loss: 0.5667 - acc: 0.7041\n",
      "Epoch 16/100\n",
      "16092/16092 [==============================] - 5s 299us/step - loss: 0.5659 - acc: 0.7056\n",
      "Epoch 17/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5630 - acc: 0.7084 0s - loss: 0.5623 - \n",
      "Epoch 18/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5625 - acc: 0.7085\n",
      "Epoch 19/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5600 - acc: 0.7105\n",
      "Epoch 20/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5587 - acc: 0.7115\n",
      "Epoch 21/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5579 - acc: 0.7127\n",
      "Epoch 22/100\n",
      "16092/16092 [==============================] - 5s 280us/step - loss: 0.5574 - acc: 0.7141\n",
      "Epoch 23/100\n",
      "16092/16092 [==============================] - 4s 279us/step - loss: 0.5543 - acc: 0.7159\n",
      "Epoch 24/100\n",
      "16092/16092 [==============================] - 4s 270us/step - loss: 0.5532 - acc: 0.7136\n",
      "Epoch 25/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5538 - acc: 0.7161\n",
      "Epoch 26/100\n",
      "16092/16092 [==============================] - 5s 283us/step - loss: 0.5521 - acc: 0.7174\n",
      "Epoch 27/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5503 - acc: 0.7194\n",
      "Epoch 28/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5499 - acc: 0.7182\n",
      "Epoch 29/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5491 - acc: 0.7196\n",
      "Epoch 30/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5470 - acc: 0.7212\n",
      "Epoch 31/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5461 - acc: 0.7215\n",
      "Epoch 32/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5459 - acc: 0.7256\n",
      "Epoch 33/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5440 - acc: 0.7240\n",
      "Epoch 34/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5441 - acc: 0.7237\n",
      "Epoch 35/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5428 - acc: 0.7225 0s - loss: 0.5442 - acc: 0.\n",
      "Epoch 36/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5415 - acc: 0.7209\n",
      "Epoch 37/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5418 - acc: 0.7241\n",
      "Epoch 38/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5409 - acc: 0.7247\n",
      "Epoch 39/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5404 - acc: 0.7240\n",
      "Epoch 40/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5387 - acc: 0.7271\n",
      "Epoch 41/100\n",
      "16092/16092 [==============================] - 4s 273us/step - loss: 0.5400 - acc: 0.7278\n",
      "Epoch 42/100\n",
      "16092/16092 [==============================] - 5s 280us/step - loss: 0.5392 - acc: 0.7243\n",
      "Epoch 43/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5381 - acc: 0.7274\n",
      "Epoch 44/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5366 - acc: 0.7278\n",
      "Epoch 45/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5363 - acc: 0.7268\n",
      "Epoch 46/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5359 - acc: 0.7274 0s - loss: 0.5363 - a\n",
      "Epoch 47/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5355 - acc: 0.7289\n",
      "Epoch 48/100\n",
      "16092/16092 [==============================] - 4s 273us/step - loss: 0.5353 - acc: 0.7297\n",
      "Epoch 49/100\n",
      "16092/16092 [==============================] - 4s 270us/step - loss: 0.5345 - acc: 0.7294\n",
      "Epoch 50/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5343 - acc: 0.7281\n",
      "Epoch 51/100\n",
      "16092/16092 [==============================] - 5s 296us/step - loss: 0.5341 - acc: 0.7277\n",
      "Epoch 52/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5325 - acc: 0.7309\n",
      "Epoch 53/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5316 - acc: 0.7289\n",
      "Epoch 54/100\n",
      "16092/16092 [==============================] - 5s 282us/step - loss: 0.5318 - acc: 0.7305\n",
      "Epoch 55/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5306 - acc: 0.7300\n",
      "Epoch 56/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5299 - acc: 0.7308\n",
      "Epoch 57/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5287 - acc: 0.7322\n",
      "Epoch 58/100\n",
      "16092/16092 [==============================] - 4s 270us/step - loss: 0.5287 - acc: 0.7325\n",
      "Epoch 59/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5275 - acc: 0.7334\n",
      "Epoch 60/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5272 - acc: 0.7348\n",
      "Epoch 61/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5268 - acc: 0.7345 \n",
      "Epoch 62/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5262 - acc: 0.7361\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5242 - acc: 0.7351\n",
      "Epoch 64/100\n",
      "16092/16092 [==============================] - 4s 270us/step - loss: 0.5240 - acc: 0.7333\n",
      "Epoch 65/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5240 - acc: 0.7363\n",
      "Epoch 66/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5234 - acc: 0.7369\n",
      "Epoch 67/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5226 - acc: 0.7373\n",
      "Epoch 68/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5224 - acc: 0.7363\n",
      "Epoch 69/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5215 - acc: 0.7376\n",
      "Epoch 70/100\n",
      "16092/16092 [==============================] - 4s 270us/step - loss: 0.5211 - acc: 0.7385\n",
      "Epoch 71/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5208 - acc: 0.7363\n",
      "Epoch 72/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5194 - acc: 0.7385\n",
      "Epoch 73/100\n",
      "16092/16092 [==============================] - 4s 273us/step - loss: 0.5191 - acc: 0.7386\n",
      "Epoch 74/100\n",
      "16092/16092 [==============================] - 4s 270us/step - loss: 0.5182 - acc: 0.7383\n",
      "Epoch 75/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5188 - acc: 0.7396\n",
      "Epoch 76/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5180 - acc: 0.7384\n",
      "Epoch 77/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5170 - acc: 0.7381\n",
      "Epoch 78/100\n",
      "16092/16092 [==============================] - 4s 270us/step - loss: 0.5164 - acc: 0.7401\n",
      "Epoch 79/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5154 - acc: 0.7409\n",
      "Epoch 80/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5153 - acc: 0.7413\n",
      "Epoch 81/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5156 - acc: 0.7401\n",
      "Epoch 82/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5149 - acc: 0.7432\n",
      "Epoch 83/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5141 - acc: 0.7429\n",
      "Epoch 84/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5138 - acc: 0.7434\n",
      "Epoch 85/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5135 - acc: 0.7422\n",
      "Epoch 86/100\n",
      "16092/16092 [==============================] - 5s 313us/step - loss: 0.5131 - acc: 0.7429\n",
      "Epoch 87/100\n",
      "16092/16092 [==============================] - 4s 278us/step - loss: 0.5140 - acc: 0.7420\n",
      "Epoch 88/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5116 - acc: 0.7457\n",
      "Epoch 89/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5120 - acc: 0.7443\n",
      "Epoch 90/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5114 - acc: 0.7432\n",
      "Epoch 91/100\n",
      "16092/16092 [==============================] - 4s 270us/step - loss: 0.5101 - acc: 0.7444\n",
      "Epoch 92/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5107 - acc: 0.7460\n",
      "Epoch 93/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5105 - acc: 0.7455\n",
      "Epoch 94/100\n",
      "16092/16092 [==============================] - 4s 270us/step - loss: 0.5090 - acc: 0.7458\n",
      "Epoch 95/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5093 - acc: 0.7477\n",
      "Epoch 96/100\n",
      "16092/16092 [==============================] - 4s 272us/step - loss: 0.5087 - acc: 0.7461\n",
      "Epoch 97/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5094 - acc: 0.7458\n",
      "Epoch 98/100\n",
      "16092/16092 [==============================] - 4s 271us/step - loss: 0.5071 - acc: 0.7491\n",
      "Epoch 99/100\n",
      "16092/16092 [==============================] - 4s 270us/step - loss: 0.5083 - acc: 0.7479\n",
      "Epoch 100/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5077 - acc: 0.7481\n",
      "1788/1788 [==============================] - 1s 327us/step\n",
      "Epoch 1/100\n",
      "16092/16092 [==============================] - 6s 391us/step - loss: 0.6349 - acc: 0.6472\n",
      "Epoch 2/100\n",
      "16092/16092 [==============================] - 5s 339us/step - loss: 0.6174 - acc: 0.6626 0s - loss: 0.6170 - acc: 0.6\n",
      "Epoch 3/100\n",
      "16092/16092 [==============================] - 5s 289us/step - loss: 0.6108 - acc: 0.6690\n",
      "Epoch 4/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.6042 - acc: 0.6721\n",
      "Epoch 5/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5965 - acc: 0.6764\n",
      "Epoch 6/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5925 - acc: 0.6812\n",
      "Epoch 7/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5879 - acc: 0.6900\n",
      "Epoch 8/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5823 - acc: 0.6916\n",
      "Epoch 9/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5789 - acc: 0.6960\n",
      "Epoch 10/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5735 - acc: 0.6985\n",
      "Epoch 11/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5702 - acc: 0.7038\n",
      "Epoch 12/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5686 - acc: 0.7044\n",
      "Epoch 13/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5658 - acc: 0.7081\n",
      "Epoch 14/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5644 - acc: 0.7098 \n",
      "Epoch 15/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5625 - acc: 0.7112\n",
      "Epoch 16/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5611 - acc: 0.7087\n",
      "Epoch 17/100\n",
      "16092/16092 [==============================] - 5s 289us/step - loss: 0.5598 - acc: 0.7128\n",
      "Epoch 18/100\n",
      "16092/16092 [==============================] - 5s 282us/step - loss: 0.5586 - acc: 0.7127\n",
      "Epoch 19/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5573 - acc: 0.7139\n",
      "Epoch 20/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5551 - acc: 0.7166\n",
      "Epoch 21/100\n",
      "16092/16092 [==============================] - 5s 284us/step - loss: 0.5550 - acc: 0.7161\n",
      "Epoch 22/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5532 - acc: 0.7151\n",
      "Epoch 23/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5522 - acc: 0.7168\n",
      "Epoch 24/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5511 - acc: 0.7196\n",
      "Epoch 25/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5502 - acc: 0.7168\n",
      "Epoch 26/100\n",
      "16092/16092 [==============================] - 4s 280us/step - loss: 0.5476 - acc: 0.7196\n",
      "Epoch 27/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5487 - acc: 0.7192\n",
      "Epoch 28/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5468 - acc: 0.7184\n",
      "Epoch 29/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5453 - acc: 0.7248\n",
      "Epoch 30/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5446 - acc: 0.7237\n",
      "Epoch 31/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5447 - acc: 0.7220\n",
      "Epoch 32/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5428 - acc: 0.7259\n",
      "Epoch 33/100\n",
      "16092/16092 [==============================] - 5s 285us/step - loss: 0.5417 - acc: 0.7244\n",
      "Epoch 34/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5414 - acc: 0.7229\n",
      "Epoch 35/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5407 - acc: 0.7289\n",
      "Epoch 36/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5398 - acc: 0.7256\n",
      "Epoch 37/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5384 - acc: 0.7273\n",
      "Epoch 38/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5368 - acc: 0.7322\n",
      "Epoch 39/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5369 - acc: 0.7274\n",
      "Epoch 40/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5345 - acc: 0.7281\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5346 - acc: 0.7297\n",
      "Epoch 42/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5328 - acc: 0.7333\n",
      "Epoch 43/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5323 - acc: 0.7323\n",
      "Epoch 44/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5310 - acc: 0.7326\n",
      "Epoch 45/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5305 - acc: 0.7311\n",
      "Epoch 46/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5299 - acc: 0.7330\n",
      "Epoch 47/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5282 - acc: 0.7338\n",
      "Epoch 48/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5284 - acc: 0.7340\n",
      "Epoch 49/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5267 - acc: 0.7351 0s - loss: 0.5259 - a\n",
      "Epoch 50/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5256 - acc: 0.7360\n",
      "Epoch 51/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5253 - acc: 0.7373\n",
      "Epoch 52/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5244 - acc: 0.7405\n",
      "Epoch 53/100\n",
      "16092/16092 [==============================] - 4s 279us/step - loss: 0.5246 - acc: 0.7367\n",
      "Epoch 54/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5229 - acc: 0.7370\n",
      "Epoch 55/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5237 - acc: 0.7365\n",
      "Epoch 56/100\n",
      "16092/16092 [==============================] - 5s 280us/step - loss: 0.5222 - acc: 0.7385\n",
      "Epoch 57/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5213 - acc: 0.7353\n",
      "Epoch 58/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5202 - acc: 0.7386\n",
      "Epoch 59/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5187 - acc: 0.7374\n",
      "Epoch 60/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5196 - acc: 0.7386\n",
      "Epoch 61/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5181 - acc: 0.7428\n",
      "Epoch 62/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5179 - acc: 0.7385\n",
      "Epoch 63/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5181 - acc: 0.7401\n",
      "Epoch 64/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5169 - acc: 0.7430\n",
      "Epoch 65/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5151 - acc: 0.7421\n",
      "Epoch 66/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5158 - acc: 0.7403\n",
      "Epoch 67/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5152 - acc: 0.7424\n",
      "Epoch 68/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5149 - acc: 0.7417\n",
      "Epoch 69/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5146 - acc: 0.7453\n",
      "Epoch 70/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5129 - acc: 0.7451\n",
      "Epoch 71/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5128 - acc: 0.7448\n",
      "Epoch 72/100\n",
      "16092/16092 [==============================] - 5s 297us/step - loss: 0.5136 - acc: 0.7457\n",
      "Epoch 73/100\n",
      "16092/16092 [==============================] - 5s 283us/step - loss: 0.5123 - acc: 0.7458\n",
      "Epoch 74/100\n",
      "16092/16092 [==============================] - 5s 284us/step - loss: 0.5124 - acc: 0.7452\n",
      "Epoch 75/100\n",
      "16092/16092 [==============================] - 5s 286us/step - loss: 0.5111 - acc: 0.7453\n",
      "Epoch 76/100\n",
      "16092/16092 [==============================] - 5s 283us/step - loss: 0.5115 - acc: 0.7457\n",
      "Epoch 77/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5116 - acc: 0.7453\n",
      "Epoch 78/100\n",
      "16092/16092 [==============================] - 5s 287us/step - loss: 0.5110 - acc: 0.7465\n",
      "Epoch 79/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5095 - acc: 0.7461\n",
      "Epoch 80/100\n",
      "16092/16092 [==============================] - 4s 278us/step - loss: 0.5106 - acc: 0.7468\n",
      "Epoch 81/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5106 - acc: 0.7448\n",
      "Epoch 82/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5097 - acc: 0.7483\n",
      "Epoch 83/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5089 - acc: 0.7471\n",
      "Epoch 84/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5083 - acc: 0.7490\n",
      "Epoch 85/100\n",
      "16092/16092 [==============================] - 5s 313us/step - loss: 0.5080 - acc: 0.7471\n",
      "Epoch 86/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5078 - acc: 0.7495\n",
      "Epoch 87/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5073 - acc: 0.7517\n",
      "Epoch 88/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5072 - acc: 0.7493 1s - los\n",
      "Epoch 89/100\n",
      "16092/16092 [==============================] - 4s 277us/step - loss: 0.5069 - acc: 0.7481\n",
      "Epoch 90/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5065 - acc: 0.7501\n",
      "Epoch 91/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5060 - acc: 0.7484\n",
      "Epoch 92/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5048 - acc: 0.7506\n",
      "Epoch 93/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5057 - acc: 0.7503\n",
      "Epoch 94/100\n",
      "16092/16092 [==============================] - 4s 278us/step - loss: 0.5054 - acc: 0.7502\n",
      "Epoch 95/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5052 - acc: 0.7527\n",
      "Epoch 96/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5041 - acc: 0.7502\n",
      "Epoch 97/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5037 - acc: 0.7510\n",
      "Epoch 98/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5038 - acc: 0.7537\n",
      "Epoch 99/100\n",
      "16092/16092 [==============================] - 4s 275us/step - loss: 0.5032 - acc: 0.7525\n",
      "Epoch 100/100\n",
      "16092/16092 [==============================] - 4s 276us/step - loss: 0.5024 - acc: 0.7524\n",
      "1788/1788 [==============================] - 1s 345us/step\n"
     ]
    }
   ],
   "source": [
    "# Applying k-Fold Cross Validation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier as kc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "classifier = kc(build_fn = build_classifier, batch_size = 10, epochs = 100)\n",
    "accuracies = cross_val_score(estimator=classifier, X = X_train, y= y_train, cv=10, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking bais variance tradeoffs\n",
      "accuracies mean: 0.615771816462545\n",
      "accuracies std: 0.027076513732631396\n"
     ]
    }
   ],
   "source": [
    "print('Checking bais variance tradeoffs')\n",
    "print('accuracies mean: ' + str(accuracies.mean()))\n",
    "print('accuracies std: ' + str(accuracies.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
